During the course of my studies several invaluable experiences have greatly contributed to my desire to pursue a research-based career in Computer Science. I have had the opportunity to participate in stimulating research projects, in such areas as Machine Learning or Signal Processing.
% As a result, I have obtained a diverse research background which I seek to put in practice through a challenging and thus interesting post-doctoral position at  Learning Agents Research Group. 
From my early years as an undergraduate student I have tried to keep the balance between theory and application. After three years of intensive Mathematics and Physics studies I entered TELECOM SudParis, a Telecommunication engineering school. There, on the one hand my engineering education made me comfortable with programming (C/C++, Java) and Network issues (LANs, WANs) and on the other and I personally got involved in a research project on PCA algorithms which has lead to a publication at ICASSP 2009. In 2008, I continued with my graduate studies in Applied Mathematics as a master student with focus on Statistical Learning where I developed solid background Machine Learning theory (including a course on Graphical Models by Francis Bach and one on Reinforcement Learning by Rémi Munos). Still I completed my master with an internship at INRIA research lab where I applied statistical learning techniques to help design a realistic automatic ad-server for Orange Inc affiliated websites. This work has launched a collaboration which is still in progress.
 
My current research involves the investigation of machine learning techniques to create algorithms that, in some way, adapts to its users, or more generally learns from its environment. The approach is both theoretical and application oriented. A major objective in our algorithms development is to ensure our algorithms capture the real complexity of a problem and testing in practice their performances in real world problems. During my PhD, I investigated Reinforcement Learning (RL) which is a field where one tries to solve complex systems where an agent has to learn from its environment. More precisely, the focus was on a class of algorithms called ``Classification-based Policy Iteration'' (CBPI) which are algorithms that learn directly the policies as output of a classifier. Thus they avoid, as in the standard RL techniques, to define a policy through an associated value function as this value function is often poorly approximated. Therefore, this class of algorithms is expected to perform better than its value-based counterparts whenever the policies are easier to represent than their value functions. However, CBPI algorithms can require large number of samples from the environment. To improve the CBPI efficiency, I proposed new hybrid approaches using value function approximations in the CBPI framework that leverage the benefits of both approaches (which led to two publications in ICML 2011 \& 2012 while a journal paper has been published in JMLR). Moreover, we applied our techniques in the game of Tetris, a domain where RL techniques had obtained poor results, and learned a controller removing on average 50.000.000 lines (the best in the literature, to the best of our knowledge which is reported in a paper in NIPS 2013).

I also investigated Bandit problems. Bandit problems are core problems to model any problem involving adaptiveness. We designed a sampling strategy to solve several bandit problems in parallel (which led to two publications in NIPS 2011 \& 2012).

During the course of my Ph.D. I worked as an research intern for 6 months at Technicolor Labs in Palo Alto California under the supervision of Branislav Kveton.  Our primary goal was to improve the questionnaire asked to elicit movie preferences of users for a recommendation website. The problem was cast as an adaptive submodular maximization problem. The novelty was that we consider this problem in the case where the preferences of the users are not supposed to be known to build the questionnaire but need to be learned (which led to a publication in NIPS 2013).

As a post-doctorate in the Queensland University of Technology, under the supervision of Peter Bartlett, I am conducting research in online learning.  My first project deals with a combinatorial set of possible choices, is set in a stochastic setting and could model network routing problem (online shortest-path problem). The second one is set in the non-stochastic setting (adversarial) where the goal is to give a simple setting of this bandit game that admits an exact mimimax solution. This therefore is a more theoretical question that draws connection with game theory.

Through the experiences already described I developed my ability to work in a team environment. The international conferences, internships and summer schools I have been attending gave me the opportunity to learn and exchange with researchers from diverse horizons. In addition, teaching computer science (Algorithmic with Python \& Databases) for Master and Licence students keeps enriching my communication skills. I build up my programming skills through my curriculum in a telecommunication engineering school and later through the lectures and practical sections I gave. Moreover most of my projects have involved programming part which have made me comfortable with coding in Python and  C++.

My long term career goal is to become a  researcher.  I wish to gain professional experience at an environment that will allow me to expand my knowledge and capabilities through collaborations with researchers who can mentor and inspire me. I am confident that GRASP will provide me with such an environment and much more. It fits my willingness to  lead research that can find real applications, particularly in artificial intelligence for games or robotic purpose (as I already worked on the Tetris game). I believe that my background in Machine Learning will permit me to take on GRASP challenge on designing powerful lifelong learning algorithms. I also believe that my diverse research background, and my prior exposure to similar research environments make me a unique candidate for the internship program at GRASP. I look forward to conducting research at GRASP  world-class research environment, while nurturing my innovative and practical abilities.
 \begin{center} \textbf{Curriculum Vitae of the Applicant, Dr Victor Gabillon}  \end{center}
 
\noindent\textbf{Education}\\[-.4cm]\noindent\makebox[\linewidth]{\rule{\columnwidth}{0.4pt}}
\begin{vitem}{June 2014 }{PhD in Computer Science}
 in Team SequeL, INRIA Lille - Nord Europe, France.
%Defended on June 12, 2014\\
\textit{Title:} ``Budgeted Classification-based Policy Iteration''\\
Domains: Reinforcement learning \& Bandits games\\
Supervisors: Mohammad Ghavamzadeh \& Philippe Preux

 \end{vitem}


 \begin{vitem}{2008-09}{M.Sc. in applied mathematics, École Normale Supérieure, Cachan, France.}
Cursus MVA (image processing \& statistical learning) with honours.\\
Relevant courses: Reinforcement Learning, Graphical Models, Statistical learning (SVM, Boosting...).
 \end{vitem}

 \begin{vitem}{2006-08}{Engineering degree, TELECOM SudParis, Évry, France.}

Graduate school of engineering committed to the development of information technology. \\
Relevant courses: Programming, Statistics, Information Theory, Image Processing, LANs \& WANs.
 \end{vitem}
 
\subsection{Publications}
 Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson $\&$ S. Muthukrishnan, \textbf{\emph{Large Scale Optimistic Adaptive Submodularity}}.
AAAI $2014$, $28^{th}$ Conference on Artificial Intelligence.
Oral presentation at Quebec City, Canada, July $2014$.


 Victor Gabillon, Mohammad Ghavamzadeh $\&$ Bruno Scherrer, 
\textbf{\emph{Approximate Dynamic Programming Finally Performs Well in the Game of Tetris}}.
NIPS $2013$, $27^{th}$ Conference on Neural Information Processing Systems.
Poster presentation at South Lake Tahoe, Nevada, December $2013$.


 Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson $\&$ S. Muthukrishnan, \textbf{\emph{Adaptive Submodular Maximization in Bandit Setting}}.
NIPS $2013$, $27^{th}$ Conference on Neural Information Processing Systems.
Poster presentation at South Lake Tahoe, Nevada, December $2013$.


 Victor Gabillon, Mohammad Ghavamzadeh $\&$  Alessandro Lazaric, \textbf{\emph{Best Arm Identification: A unified approch to fixed budget and fixed confidence}}.
NIPS $2012$, $26^{th}$ Conference on Neural Information Processing Systems.
Poster presentation at South Lake Tahoe, Nevada, December $2012$.


 Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon $\&$ Matthieu Geist, \textbf{\emph{Approximate Modified Policy Iteration}}.
ICML $2012$, $29^{th}$  International Conference on Machine Learning.
Long lecture presentation at Edinburgh, Scotland, June $2012$.


 Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric $\&$ Sébastien Bubeck, \textbf{\emph{Multi-Bandit Best Arm Identification}}.
NIPS $2011$, $25^{th}$ Conference on Neural Information Processing Systems.
Poster presentation at Granada, Spain, December $2011$.


 Victor Gabillon, Alessandro Lazaric, Mohammad Ghavamzadeh $\&$  Bruno Scherrer, \textbf{ \emph{Classification-based Policy Iteration with a Critic}}. ICML $2011$, $28^{th}$  International Conference on Machine Learning. Lecture presentation at Bellevue, USA, June $2011$.


 Victor Gabillon,  Alessandro Lazaric, Mohammad Ghavamzadeh \textbf{ \emph{Rollout Allocation Strategies for Classification-based Policy Iteration}}. Workshop on Reinforcement Learning and Search in Very Large Spaces International Conference on Machine Learning,  Lecture presentation at Haifa, Israel, June $2010$.


 Victor Gabillon, Jérémie Mary $\&$ Philippe Preux, \textbf{ \emph{Affichage de publicités sur des portails web}}. EGC $2010$, $10^{th}$ French-speaking International Conference on Knowledge Extraction and Management. Lecture presentation of long article at Hammamet, Tunisia, January $2010$. Best applied paper award.

 
 Jean-Pierre Delmas $\&$ Victor Gabillon,\textbf{ \emph{Asymptotic performance analysis of PCA algorithms based on the weighted subspace criterion}}.  ICASSP $2009$, International Conference on Acoustics, Speech and Signal Processing. Poster presentation at Taipei, Taiwan, April $2009$. 
   

  Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon $\&$ Matthieu Geist, \textbf{\emph{Approximate Modified Policy Iteration}}, JMLR.

