During the course of my studies several invaluable experiences have greatly contributed to my desire to pursue a research-based career in Computer Science. I have had the opportunity to participate in stimulating research projects, in such areas as Machine Learning or Signal Processing.
% As a result, I have obtained a diverse research background which I seek to put in practice through a challenging and thus interesting post-doctoral position at  Learning Agents Research Group. 
From my early years as an undergraduate student I have tried to keep the balance between theory and application. After three years of intensive Mathematics and Physics studies I entered TELECOM SudParis, a Telecommunication engineering school. There, on the one hand my engineering education made me comfortable with programming (C/C++, Java) and Network issues (LANs, WANs) and on the other and I personally got involved in a research project on PCA algorithms which has lead to a publication at ICASSP 2009. In 2008, I continued with my graduate studies in Applied Mathematics as a master student with focus on Statistical Learning where I developed solid background Machine Learning theory (including a course on Graphical Models by Francis Bach and one on Reinforcement Learning by Rémi Munos). Still I completed my master with an internship at INRIA research lab where I applied statistical learning techniques to help design a realistic automatic ad-server for Orange Inc affiliated websites. This work has launched a collaboration which is still in progress.
 
My current research involves the investigation of machine learning techniques to create algorithms that, in some way, adapts to its users, or more generally learns from its environment. The approach is both theoretical and application oriented. A major objective in our algorithms development is to ensure our algorithms capture the real complexity of a problem and testing in practice their performances in real world problems. During my PhD, I investigated Reinforcement Learning (RL) which is a field where one tries to solve complex systems where an agent has to learn from its environment. More precisely, the focus was on a class of algorithms called ``Classification-based Policy Iteration'' (CBPI) which are algorithms that learn directly the policies as output of a classifier. Thus they avoid, as in the standard RL techniques, to define a policy through an associated value function as this value function is often poorly approximated. Therefore, this class of algorithms is expected to perform better than its value-based counterparts whenever the policies are easier to represent than their value functions. However, CBPI algorithms can require large number of samples from the environment. To improve the CBPI efficiency, I proposed new hybrid approaches using value function approximations in the CBPI framework that leverage the benefits of both approaches (which led to two publications in ICML 2011 \& 2012 while a journal paper has been published in JMLR). Moreover, we applied our techniques in the game of Tetris, a domain where RL techniques had obtained poor results, and learned a controller removing on average 50.000.000 lines (the best in the literature, to the best of our knowledge which is reported in a paper in NIPS 2013).

I also investigated Bandit problems. Bandit problems are core problems to model any problem involving adaptiveness. We designed a sampling strategy to solve several bandit problems in parallel (which led to two publications in NIPS 2011 \& 2012).

During the course of my Ph.D. I worked as an research intern for 6 months at Technicolor Labs in Palo Alto California under the supervision of Branislav Kveton.  Our primary goal was to improve the questionnaire asked to elicit movie preferences of users for a recommendation website. The problem was cast as an adaptive submodular maximization problem. The novelty was that we consider this problem in the case where the preferences of the users are not supposed to be known to build the questionnaire but need to be learned (which led to a publication in NIPS 2013).

As a post-doctorate in the Queensland University of Technology, under the supervision of Peter Bartlett, I am conducting research in online learning.  My first project deals with a combinatorial set of possible choices, is set in a stochastic setting and could model network routing problem (online shortest-path problem). The second one is set in the non-stochastic setting (adversarial) where the goal is to give a simple setting of this bandit game that admits an exact mimimax solution. This therefore is a more theoretical question that draws connection with game theory.

Through the experiences already described I developed my ability to work in a team environment. The international conferences, internships and summer schools I have been attending gave me the opportunity to learn and exchange with researchers from diverse horizons. In addition, teaching computer science (Algorithmic with Python \& Databases) for Master and Licence students keeps enriching my communication skills. I build up my programming skills through my curriculum in a telecommunication engineering school and later through the lectures and practical sections I gave. Moreover most of my projects have involved programming part which have made me comfortable with coding in Python and  C++.

My long term career goal is to become a  researcher.  I wish to gain professional experience at an environment that will allow me to expand my knowledge and capabilities through collaborations with researchers who can mentor and inspire me. I am confident that GRASP will provide me with such an environment and much more. It fits my willingness to  lead research that can find real applications, particularly in artificial intelligence for games or robotic purpose (as I already worked on the Tetris game). I believe that my background in Machine Learning will permit me to take on GRASP challenge on designing powerful lifelong learning algorithms. I also believe that my diverse research background, and my prior exposure to similar research environments make me a unique candidate for the internship program at GRASP. I look forward to conducting research at GRASP  world-class research environment, while nurturing my innovative and practical abilities.
 \begin{center} \textbf{Curriculum Vitae of the Applicant, Dr Victor Gabillon}  \end{center}
 
\noindent\textbf{Education}\\[-.4cm]\noindent\makebox[\linewidth]{\rule{\columnwidth}{0.4pt}}\\[.1cm]
\noindent\begin{tabularx}{\columnwidth}{@{} l X @{}}
\noindent\textbf{PhD in Computer Science} (Accessit Award of the AI French Association, AFIA)& \hfill \textbf{June 2014} \\
Team SequeL, INRIA Lille - Nord Europe, France\\
%Defended on June 12, 2014\\
\textit{Title:} ``Budgeted Classification-based Policy Iteration''\\
\textit{Domains:} Reinforcement learning \& Bandits games\\
\textit{Supervisors:}  Mohammad Ghavamzadeh \&  Philippe Preux\\
\noindent\textit{Examiners:}\begin{tabular}{ll}  &Peter Auer (Leoben University), Olivier Cappé   (Télécom ParisTech), \\
\noindent & Shie Mannor   (Technion)  and Csaba Szepesvári  (Alberta  University)  
\end{tabular}\\[.2cm]
\textbf{M.Sc. Image Processing \& Statistical Learning} with honours &\hfill \textbf{ Sep 2009}\\
 École Normale Supérieure, Cachan, France\\[.2cm]
\textbf{Engineering degree in information technology}  &\hfill \textbf{ Sep. 2009}\\
 TELECOM SudParis, Évry, France
\end{tabularx}\\[.2cm]

\noindent\textbf{Professional Activities}\\[-.4cm]\noindent\makebox[\linewidth]{\rule{\columnwidth}{0.4pt}}\\[.1cm]
\noindent\textbf{Postdoctoral Research Fellow in Statistics} \textit{full time } \hfill \textbf{Nov 2015 -- ongoing} \\
School of Mathematical Sciences, Queensland University of Technology, Brisbane, Australia\\
\noindent\textbf{PhD Researcher} \textit{full time } \hfill \textbf{Oct 2009 -- June 2014}\\
Team SequeL, INRIA Lille - Nord Europe, France\\
\textbf{Research Engineer}  \textit{full time }  \hfill \textbf{ Mar 2013 -- Sep 2013}\\
Technicolor Research Group, Palo Alto, USA.\\
\textbf{External Lecturer} \textit{part time } \hfill \textbf{ Fall 2012}\\
Lille 1 University, France\\
\textbf{External Lecturer} \textit{part time }\hfill \textbf{2010 -- 2011  }\\
Lille 3 University, France\\ 
\textbf{Research Engineer}  \textit{full time }  \hfill \textbf{ June 2008 -- Sep 2008}\\
Chinese Academy of Science, Beijing, China.\\[.2cm] 

\noindent\textbf{Awards \& Grants}\\[-.4cm]\noindent\makebox[\linewidth]{\rule{\columnwidth}{0.4pt}}\\[.1cm]
\noindent\textbf{Postdoctoral Research Fellowship in Statistics} \hfill \textbf{Nov 2015} \\
Two-year fellowship funded by the Queensland University of Technology\\
\noindent\textbf{Second place award for the best French PhD in Artificial Intelligence }  \hfill \textbf{June 2015}\\
Award from AFIA, the French Association for Artificial Intelligence.\\
\textbf{Best applied paper award}  \hfill \textbf{ Jan 2010}\\
Award from the EGC conference, French speaking conference on knowledge mining and management.\\
\textbf{PhD Grant}  \hfill \textbf{ Oct 2009}\\
Three-year grant funded by the French Ministry of Research\\[.2cm]

\noindent\textbf{Research Expeditions}\\[-.4cm]\noindent\makebox[\linewidth]{\rule{\columnwidth}{0.4pt}}\\[.1cm]
\noindent\textbf{3 months at Berkeley Statistic Departement, USA} \hfill \textbf{Mar -- June 2015} \\
Hosted by Peter Bartlett\\
\noindent\textbf{One week at Inria Nancy-Grand Est, France} \hfill \textbf{June 2012} \\
Hosted by Bruno Scherrer of the team Maia\\

\noindent\textbf{Peer Reviewer}\\[-.4cm]\noindent\makebox[\linewidth]{\rule{\columnwidth}{0.4pt}}\\[.1cm]
I have been an official reviewer for the Neural Information Processing Systems (NIPS) international conference in 2014 and 2015 and I have reviewed papers for the Machine Learning Jounal and the Journal of Machine Leaning Research (JMLR).\\


\noindent 
\textbf{Invited Presentations}
\\[-.4cm]\noindent\makebox[\linewidth]{\rule{\columnwidth}{0.4pt}}\\[.1cm]
\textbf{\textit{Talks other than Conference presentations}}\\
\textbf{Talk} Oxford Robotics Research Group Seminar, Oxford, UK, May 2014\\
 ``Classification-Based Policy Iteration perform well in the game of Tetris''.\\
\noindent \textbf{Talk} Gatsby Reinforcement Learning Research Group, London, UK, May 2014\\
 ``Classification-Based Policy Iteration perform well in the game of Tetris''.
  
\noindent \textbf{Talk} Team Maia Seminar, Nancy, France, June 2012\\
 ``Pure Exploration Bandits''.
 
\noindent \textbf{Talk} Co-Adapt Seminars, Marseille, France,  May 2012\\
 ``Pure Exploration Bandits for Brain-Computer Interface?''.\\

\noindent\textbf{Publications}\\[-.4cm]\noindent\makebox[\linewidth]{\rule{\columnwidth}{0.4pt}}\\[.1cm]
\textit{\textbf{Peer-reviewed journal article}}\\
\noindent\begin{tabularx}{\columnwidth}{@{} l X @{}}
 J1. & Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon $\&$ Matthieu Geist, \textbf{\emph{Approximate Modified Policy Iteration}}, to appear in Journal of Machine Learning Research (JMLR).
  \end{tabularx}\\
  
\noindent\textit{\textbf{Peer-reviewed conference article}}\\
\noindent\begin{tabularx}{\columnwidth}{@{} l X @{}}
C9. & Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson $\&$ S. Muthukrishnan, \textbf{\emph{Large Scale Optimistic Adaptive Submodularity}}.
AAAI $2014$, $28^{th}$ Conference of the Association for the Advancement of  Artificial Intelligence.
Oral presentation at Quebec City, Canada, July $2014$.\\

C8. & Victor Gabillon, Mohammad Ghavamzadeh $\&$ Bruno Scherrer, 
\textbf{\emph{Approximate Dynamic Programming Finally Performs Well in the Game of Tetris}}.
NIPS $2013$, $27^{th}$ Conference on Neural Information Processing Systems.
Poster presentation at South Lake Tahoe, Nevada, December $2013$.\\


C7. & Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson $\&$ S. Muthukrishnan, \textbf{\emph{Adaptive Submodular Maximization in Bandit Setting}}.
NIPS $2013$, $27^{th}$ Conference on Neural Information Processing Systems.
Poster presentation at South Lake Tahoe, Nevada, December $2013$.\\


C6. & Victor Gabillon, Mohammad Ghavamzadeh $\&$  Alessandro Lazaric, \textbf{\emph{Best Arm Identification: A unified approch to fixed budget and fixed confidence}}.
NIPS $2012$, $26^{th}$ Conference on Neural Information Processing Systems.
Poster presentation at South Lake Tahoe, Nevada, December $2012$.\\


C5. & Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon $\&$ Matthieu Geist, \textbf{\emph{Approximate Modified Policy Iteration}}.
ICML $2012$, $29^{th}$  International Conference on Machine Learning.
Long lecture presentation at Edinburgh, Scotland, June $2012$.\\


C4. & Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric $\&$ Sébastien Bubeck, \textbf{\emph{Multi-Bandit Best Arm Identification}}.
NIPS $2011$, $25^{th}$ Conference on Neural Information Processing Systems.
Poster presentation at Granada, Spain, December $2011$.\\


C3. & Victor Gabillon, Alessandro Lazaric, Mohammad Ghavamzadeh $\&$  Bruno Scherrer, \textbf{ \emph{Classification-based Policy Iteration with a Critic}}. ICML $2011$, $28^{th}$  International Conference on Machine Learning. Lecture presentation at Bellevue, USA, June $2011$.\\

C2. & Victor Gabillon, Jérémie Mary $\&$ Philippe Preux, \textbf{ \emph{Affichage de publicités sur des portails web}}. EGC $2010$, $10^{th}$ French-speaking International Conference on Knowledge Extraction and Management. Lecture presentation of long article at Hammamet, Tunisia, January $2010$. Best applied paper award.\\

 
C1. & Jean-Pierre Delmas $\&$ Victor Gabillon,\textbf{ \emph{Asymptotic performance analysis of PCA algorithms based on the weighted subspace criterion}}.  ICASSP $2009$, International Conference on Acoustics, Speech and Signal Processing. Poster presentation at Taipei, Taiwan, April $2009$. 
   \end{tabularx}\\

\noindent\textit{\textbf{Peer-reviewed workshop article}}\\
\noindent\begin{tabularx}{\columnwidth}{@{} l X @{}}
 W1. & Victor Gabillon,  Alessandro Lazaric $\&$ Mohammad Ghavamzadeh, \textbf{ \emph{Rollout Allocation Strategies for Classification-based Policy Iteration}}. Workshop on Reinforcement Learning and Search in Very Large Spaces International Conference on Machine Learning,  Lecture presentation at Haifa, Israel, June $2010$.
  \end{tabularx}\\
  
  

\noindent\textit{\textbf{Major research achievements \& industrial innovations}}\\[-.4cm]\noindent\makebox[\linewidth]{\rule{\columnwidth}{0.4pt}}\\[.1cm]
\begin{itemize}
\item \textit{\text{Research:}} \textit{\textbf{Reinforcement Learning is finally competitive:}} We proposed a new family of reinforcement learning methods based on the ``Classification-based Policy Iteration'' algorithms. In addition to proposing theoretical analysis of this methods (C3,C5), we implemented a complex and extensive experimental studies of the performance of this algorithms in the famous benchmark of the game of Tetris. Our result show that for the first time a reinforcement learning methods performs well in Tetris even improving on the state-of-the-art techniques. Moreover, while these state-of-the-art techniques were based on black-box optimisation techniques that requires a lots of samples from the environment, our methods require 10 times less samples to learn Tetris strategies with same performance (C8).
\item \textit{\text{Industry:}} \textit{\textbf{Constrained Learning for Orange Ad Server:}} Orange, the french leading company in telecommunications had made a contract with the research team SequeL in order to turn their online web-advertising services automatic. My initial goal was to make a survey of the machine learning literature and find an appropriate solution optimizing their clic-per-rate revenues. This solution had to take into account specific new constraints on the limited and known number of display per ads. Finally, a new approach was proposed combining linear programming and bandits algorithms with experiments on synthetic data. The results was published and awarded in a french speaking conference (C2) and started a collaboration between SequeL and Orange which is still running.  
\item \textit{\text{Industry:}} \textit{\textbf{Adaptive Questionnaire Design at Technicolor Inc:}} During research and developpement internship at Technicolor, the primary goal was to improve the questionnaire asked to elicit movie preferences of users for a recommendation website. The problem was cast as an adaptive submodular maximization problem. The novelty was that we considered this problem in the case where the preferences of the users are unknown  but need to be learned in order to build an adaptive questionnaire (C7,C9).
\end{itemize}



\noindent\text{\textbf{Teaching}}\\[-.4cm]\noindent\makebox[\linewidth]{\rule{\columnwidth}{0.4pt}}\\[.1cm]
In the past 5 years I taught 216 hours of undergraduate and master's courses in France.\\
\textit{\textbf{Instructor:}} 
 \begin{itemize} 
 \item\textit{Introduction to algorithmic and programming with Python.}\\
    $48$ hours (lectures and practical sessions). Winter 2010, Fall 2011 \& Fall 2012\\
    $1^{rst}$ year of Master \textit{Computer science and document} at Lille $3$ University and  $1^{rst}$ year of Licence \textit{Physics-Chemistry} at Lille $1$ University.
    \end{itemize}
\textit{\textbf{Teaching assistant:}} 
\begin{itemize}
 \item \textit{SQL and Python.} $36$ hours (practical sessions). Fall 2010.\\
    $3^{rd}$ year of Licence \textit{Mathematics and computer science applied to social sciences} at Lille 3 University. 
\item \textit{Designing databases and object-oriented programming}.
    $36$ hours (practical sessions). Winter 2011.\\
    $3^{rd}$ year of Licence \textit{Mathematics and computer science applied to social sciences} at Lille 3 University. 
\end{itemize}

