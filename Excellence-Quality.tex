
\subsection{Quality, innovative aspects and credibility of the research (including inter/multidisciplinary aspects)}
\label{sec:quality}

A critical concern in the modern world is security. Effectively protecting the ports, airports, trains and other transportation systems from malicious attacks, fighting the trafficking of drugs and firearms, and securing proprietary and sensitive information over the ever-growing cyber-networks, comprise some of the principal axes of this critical task. The main challenge in all of these problems is that maximum security must be obtained with a limited number of available resources. For instance, the total number of security agents available to simultaneously protect a multitude of designated targets may not be sufficient to provide full security coverage at an airport. This calls for the design of appropriate resource allocation techniques which, given the available resources, would result in maximum security. %Clearly, an important feature of these methods must be to provide highly unpredictable strategies, while devising appropriate target priorities in the presence of uncertainty about the adversaries' interests.  
 
Security resource allocation and scheduling problems comprise one of the many application areas that have recently been shown to greatly benefit from game-theoretic approaches. Indeed, as a solid mathematical framework to model strategic decision making, game theory has proved useful in many real-world applications from economics and political science to logic, computer science and psychology. In this paradigm, the problem is cast as a ``game'' and the objective is to find a solution whereby each ``player'' makes choices to maximise her own \textit{utilities}, which may often be in conflict with those of her opponent. A ``security game'' corresponds to a competition between a defender and an attacker. To solve a security game, all possible actions (attacks and defences) of the two players are enumerated, and for each player an outcome (value) is assigned, which depends on the pair of actions taken by both players. In cases where these outcomes are known, game-theoretic approaches have provided impressive results. Since 2007, the so-called ARMOR software \cite{pita2008deployed} is used at the Los Angeles International Airport (LAX) to effectively determine checkpoints on the roadways leading to the airport, and to canine patrol routes within terminals. Similarly, such programs as IRIS \cite{tsai2009iris}, PROTECT \cite{shieh2012protect}, and TRUSTS \cite{yin2012trusts} are respectively being deployed at the US Federal Air Marshals, the US coast guard patrolling, and the Los Angeles Metro system's fare inspection strategy. 

A severe limitation of these models is that the true utility functions are usually unknown and must be estimated by experts or obtained from historical data. As a result, the potentially high estimation errors or the lack of historical data from which is suffering any newly established security system, will often render the security game solver useless. 
%However, the data from which these values can be obtained are usually scarce and the error in expert guesses may be high, rendering the security game solver useless. 
%A common example is when the players cannot completely foresee the outcome of some of her actions, or is bound to ignore what utilities her opponent is aiming to maximise. For instance, the exact efficacy of check-points or the drug smugglers' routes at an airport may be unknown. 
%Indeed, an important observation, forming the basis of this grant proposal, is that this {\em learning} problem lying at the heart of security games can be solved using carefully designed machine learning techniques. 
Therefore it is of importance to use methods that can quickly collect relevant data  in order to estimate the parameters of the game and quickly be operational.

Machine learning lies at the crossroad between statistics and computer science. The common goal is to design programs able to actively and intelligently gather data and extract information from the collected data, autonomously using them to make strategic decisions. Based on theoretically sound statistical methods, machine learning techniques are ubiquitously being deployed in a variety of modern applications ranging from robotics to personalised product recommendation. 

\TODO{rewrite this end!!}
 {\em Learning} is a very well fitted approach to tackle security games as they are often of repeated form, played daily between a defender and possible attackers. Repeated security games allow for the continuous collection of data, which, through data-driven approaches can in turn be used to estimate the parameters of the game. Another possibility is in fact that the defender can in turn test the game and collect (in the most efficient and less costly manner) more information about the game. The key objective forming the basis of this grant proposal, is thus to design efficient and theoretically sound, data-driven methods that can actively interact with the environment to {\em learn} a fair model through repeated games. Using machine learning, the goal will be either \textbf{1)} to limit the costs imposed by this extra need to learn the model or  \textbf{2)} to actually design an efficient data mining procedure before the game starts that guarantee a good defence strategy.

 

 

\noindent \textbf{\textit{\\State-of-the-art}}
\noindent \textbf{\textit{\\i. Security meets Game Theory }}\\
%\TODO{better distinction between minimax nash stacjkelberg}
From a game-theoretic perspective, a security problem is viewed as a two-player game that captures the interaction between a defender (e.g., border patrols, metro inspectors, network administrators) and an attacker (e.g., terrorists/drug smugglers, illegal metro users, malicious cyber attackers). The action of the defender (attacker) is defined as selecting a subset of targets to protect (attack). For each defender/attacker action pair, \textit{utilities} are defined as the players' gain or loss, and the players' objectives are to maximise their corresponding pay-offs. From the defender's perspective, this corresponds to efficiently allocating a limited number of resources to secure some predefined targets. These utilities for both players can be stored in two  matrices,  $\boldsymbol A$ for the defender,  $\boldsymbol B$ for the attacker, where $\boldsymbol A_{i,j}$ is the utility for the defender, when she plays her strategy $i$ and the attacker responds with her $j^{\text{th}}$ strategy; the utility $\boldsymbol B_{i,j}$ for the attacker is defined analogously. 
%The defender allocates a limited number of resources to secure some predefined target, which may under threat by the attacker. 
Solutions to such games rely on randomised strategies, making the defender's scheme highly unpredictable for the attacker, thus giving rise to a significant advantage over the original mechanisms that are based on deterministic human schedulers. 
Another important feature of this paradigm is that it allows to obtain theoretical guarantees. Specifically, in the case of games that are fully competitive between the two players  (i.e. the so-called zero-sum games), the solution is provably robust in that it provides guaranteed performance against {\em any} possible attacker. Moreover, such guarantees hold, even if the defender's strategy is completely revealed to the attacker.  
The extension of this guarantee to a more general (non zero-sum) game is provided by Stackelberg equilibrium, a notion that generalises the famous Nash equilibrium \cite{korzhyk2011stackelberg}. Indeed, this is well-suited to the scenario where the defender's strategies may be at risk of revelation or leak. 
%Beyond these fundamental results, 
%some questions forming the primary focus of research in security games have been scalability, robustness with respect to uncertainty on the players utilities, or devising strategies that take advantage of the attacker's potentially limited rationality or bounded memory\cite{tambe2012game}. 

%\noindent \textbf{\textit{\\Tools from Sequential decision-making under uncertainty}}\\

\noindent \textbf{\textit{\\ii. Uncertainty in Security Games}}\\
Uncertainty is an important factor to consider in most real-world applications. 
In some scenarios, the players may be uncertain about their utilities (the $A_{i,j}$'s), or those of the other players'. For instance, in the context of security games, the random selection of passengers for security checks at an airport is a source of uncertainty in this game, where the outcome is random and the probability of successful security enforcement  is unknown. 
As also confirmed by several empirical studies in fraud and cybercrime detection, this phenomenon can significantly decrease the defender's performance\cite{granick2005faking, swire2009no}.
Extensive studies have been dedicated to the design of security games that are robust with respect to uncertainty about the environment\cite{aghassi2006robust,Nguyen14RO, Kiekintveld:2013}. 
However, an important observation is that much more can be done in the case of {\em repeated} security games.  
Indeed, this repetition allows the defender to further reduce her uncertainty about the model and intelligently {\em learn} how to improve her performance over time. Specifically, in this case, a security game solver can autonomously take intelligent decisions at repeated instances of the game, by carefully collecting, extracting and acting upon information from historical data. As discussed further in the subsequent section, this is precisely the scope of machine learning tools. 

\noindent \textbf{\textit{\\iii. Indispensable Tools from Machine Learning}}\\
%Machine learning is a field of computer science where the goal is to design software able to extract information from data so that the machine itself can make use of this information to take  autonomous decisions.
%The problem of sequential decision-making arises in our everyday lives, when we seek answers to such simple questions as how to navigate to work, retrieve our desired information from the Internet, or even
%play and win at backgammon, poker or tetris (the latter was used as a testbed in the author's PhD thesis). Naturally, these situations incur uncertainty since not only are the dynamics of the environment stochastic but also because the underlying stochastic mechanism that generates these dynamics are unknown.   
%
%In the context of security games, the random selection of passengers for security checks at an airport is an example source of uncertainty in this game where the outcome is random and the probability of successful security enforcement  is unknown. 
%In these applications, a security game solver is required to autonomously take intelligent decisions at repeated instances of the game, and must thus carefully gather, extract and act upon information from historical data. 
%
Machine learning consists of data-driven techniques with strong ties to the fields of statistics (allowing them to account for uncertainty about the data) and optimisation (so as to quickly converge to the desired solution, minimising for instance, the number of actions required to achieve a specific level of performance).   	

%It has proven to make a difference in practice and has nice mathematical background. We want to focus here on two particular methods that are interesting in machine learning and where Victor and David are experts.
%First, many interesting sequential decision-making tasks can be formulated as reinforcement learning (RL) problems. In RL, an agent interacts with a dynamic, stochastic, and incompletely known environment with the goal of learning a strategy or \textit{policy} to optimize some measure of its long-term performance (e.g.,~to remove as many lines as possible in Tetris).
%talk very quickly about MDP POMDP tree search?


One of the fundamental problems in machine learning, relevant to our research objectives in this proposal, is the \textit{multi-armed bandit problem}. It corresponds to a scenario where the learner is required to actively collect data from an environment in order to solve a given task. The solutions built for this problem have found many practical applications from adaptive routing in a network to medical trials of new medicines\cite{bubeck2012regret}.  
The bandit problem is a repeated game where, at each time step the same (fixed) set of actions is available to the learner. 
In its simplest form there are $K$ actions (arms). At each round $t$, the environment allocates rewards to each arm (described as a vector $l^t\in\R^k$) while the forecaster simultaneously chooses an arm $I(t)$ to pull in order to obtain a reward $l^t_{I(t)}$. An important constraint in this setting is that the player is not allowed to observe the hypothetical reward that would have been collected had another arm been selected instead.
A very simple security game can be formulated as a bandit problem where the $K$ arms/actions correspond to the $K$ possible security strategies whose values are determined according to some unknown underlying probability distribution. 
The average per action reward can be estimated through sampling, i.e. via pulling the corresponding arm. 
%The problem has been studied under two main assumptions about the rewards. 
  
%all of the utilities are chosen by an  the defender takes actions against an attacker who adversarially designs the rewards of the game.
%For this proposal we focus on two different ways to measure the performance of the methods that corresponds to two different security requirements. % The first formulation corresponds to the classical cumulative regret setting where the forecaster tries to constantly choose the security strategy with the highest value on average.  The second one is the ``pure exploration'' setting, where the forecaster can uses the exploration phase, a limited phase during which he can freely pull the arms he chooses to, in order to identify the best  security strategy among the $K$.

%Sequential decision-making tasks is often coupled with online learning in the sense such problem needs to learn online how to solve a task while solving it. These two features, sequential decision making and online learning while be most required for our new problems right! elaborate a bit


%The problems put forward a notion of complexity of the problem. That we try to characterise well. And that we will try to study in our new problems.

While the multi-arm bandit games are extremely indispensable to active learning, an important question is how they can be used in general security games.


%\noindent \textbf{\textit{\\Frontiers of Security Games: From handling uncertainty towards self-learning algorithms}}\\
\noindent \textbf{\textit{\\iv. Existing results}}\\
%An extensively litterature has tried to  devise security games methods that are robust with respect to uncertainty about the environment\cite{aghassi2006robust,Nguyen14RO, Kiekintveld:2013}.
%Granick, for example,
%argues that weaknesses in our understanding of the measurability of losses serve as
%an impediment in sentencing cybercrime offenders\cite{granick2005faking}. Swire adds that deterring
%fraudsters and criminals online is hampered if we cannot correctly aggregate their
%offences across different jurisdictions \cite{swire2009no}. These  interest in dealing with uncertainty in the data shows how important and crucial and costly it could be and that it is a real issue in real problems.
Some interesting advancements in security games have recently been  made through links with optimisation and machine learning methods. 
These methods mostly focus on the case where the attacker's preferences are not fully known and are thus to be learned; the learning objective is achieved through a repeated a game. Some recent work analyse the number of required queries to learn the optimal defender's strategy \cite{blum2014learning, letchford2009learning}. 
Marecki et. al. and Qian et. al.\cite{Marecki12PR, qian2014online} take an empirical Bayesian approach where, given a prior distribution, planning techniques based on Partially Observable Markov Decision Processes (POMDPs) are used to update the posterior over the adversary's preferences.
The main theoretical drawback of this planning method is in that the algorithm is based on Upper Confidence Trees (UCT), which, as shown by  \cite{munos2014bandits}, are provably sub-optimal. 
Recently, an extended analysis has been given\cite{Balcan15CR} for the case of multiple attackers, where at each round of the game, a single attacker is chosen adversarially from a fixed, finite, set of known attackers. This corresponds to a case where the utility matrix $\boldsymbol B$ is chosen adversarially from a set of $k$ known matrices.  The latter work shows strong connections with adversarial bandit theory. 

%\textbf{Motivation.} %%%V what can I bring


\noindent \textit{\textbf{\\Main Goal}}\\
The main issues forming the primary focus of research in security games have been scalability, the ability for the designed methods to handle problems with very large number of actions for the players, or devising strategies that take advantage of the attacker's potentially limited rationality or bounded memory\cite{tambe2012game}.
The purpose of this proposal is to design new methods for security games that, while keeping strong scalable guaranties are even more practical  in the sense that they would be autonomous in handling the uncertainty in the model and would actively be working at reducing it by interacting with the environment in which the game takes place. We desire to broaden the scope of repeated security game problems where the initial uncertainty about the players utilities can be overcome through using learning techniques  in conjunction with  repetitive plays of the game. These techniques are from the extremely active field of machine learning research.
First we will look at new instances of these problem that are related to our area of bandit expertise and that serve real purpose.
To solve real world problems our higher priority fields of investigations will be:

\begin{itemize}
\item \textit{Explore stochastic assumption and adversarial assumption:} In a first setting, the environment chooses the rewards stochastically, based on some predetermined (however unknown) distributions. In security games this stochastic assumption helps model the unknown dynamics of the problem which both the defender and attacker can learn through interaction with the environment. In the second setting, no stochastic assumptions are made. Instead, an adversary is assumed to have arbitrarily chosen and designed the rewards
\cite{Auer03NS}. This can be used to model a more pessimistic security game where the utilities are chosen adversarially by the attacker.
%Aside from contributing to adversarial setting, one of our interest  is to make Stochastic assumptions when dealing with noise in the model and adversarial assumption when dealing with the adversary to make our approach both realistic and robust.
%%%A depending on the considered scenraio, it may make sense to consider the problem as adversarial or stochastic. The distinction will be made clear in the ... 
%So far stochastic noise in the model  in conjunction with learning has been untouched while it can happen when phenomenon are not inherently adversarial (sensor that work stochastically action that have stochastic outcomes, checkpoint might not stop deterministically the attacks and the probability of success needs to be determined, here learned).
\item \textit{Efficient learning algorithms:} 
Our goal is to have a theoretically sound approach by designing efficient algorithms for which we can provide finite sample analysis. %Mention my previous work?

\item \textit{Scalable learning algorithms:} Security solvers must typically handle a number of possible action that can be extremely large.
Simplifying structure assumption such as combinatorial structure will be considered as well as the simplifying submodular property of the objective function.

\item \textit{Different feedback structures:} The complexity of learning highly depends on the quality of the feedback that the learned can collect. From the most informative full information feedback to the many variations of partial feedback setting it is of importance, as discussed in Objective 1, to quantify how the nature of the feedback  affects the performance and to focus on scenario that correspond to real world example.

\item \textit{Robust learning:} When dealing with security game, robustness is a key issue. We insure robustness in  three different manners  to insure. First, one can be  very conservative and assume that an adversary actually chooses (in the most adversarial way) the data that we do not know. Second, we introduce offline learning  where the test and the learning happens  before the strategy is actually use in the real world. Third, we might be required to learn defence strategies that do not possess large variances in their performance. 
 %\item \textit{Adaptivity ?}
\end{itemize}


\textbf{Objective 1: Pure exploration in security games.}\\
%Contrary to the cumulative regret setting, the rewards collected before the end of the game are not taken into account.  
In many practical scenarios the defender is in fact able to safely examine her defensive strategies before applying them in the real world scenario. For example, a security system at an airport can be tested many times before being deployed as the principal defence scheme. This feature of the problem allows the learning module of the corresponding security game to be readily captured by a recent bandit setting called \textit{pure exploration}, where the learner is only evaluated at the end of an exploration phase comprising of a limited number of interactions with the environment. 
At this point, the defender's objective is to, either minimise the number of tests needed to identify an excellent strategy with a given level of confidence
(following the \textit{fixed confidence} setting\cite{Maron93HR,Even-Dar06AE}), or to maximise her probability of identifying the best strategy given a fixed number of tests (following the \textit{fixed budget} setting\cite{Bubeck09PE,Audibert10BA}).
Indeed, security games under pure exploration have not been considered in previous work, and have a strong potential to bring new and interesting angles to the domain. Note that pure exploration has also proved useful in other practical scenarios: its application to parallel action selection in robotic planning has been extensively studied by the fellow \cite{Gabillon11MB}.
%the fellow has explored the application of pure exploration bandit to parallel action seletcion in robotic planning.  
%
%is a natural addition to learning with security games,  
%
%the possibilities and limitations
%what kinds of guarantees?
%new complexity

%Security games under pure exploration bring a new angle have not been previously considered. 

%Bringing the pure exploration to security games is a natural first phase of this proposal. 
%As it is first one of the main domain of expertise of Victor and two it brings a first 
%conservative way to address learning without bringing too much risks for the learner. 
%Indeed, in this setting the learning of the unknown model happens during an exploration phase before final launch.  This for instance means that he can run tests of the security in a variety of predetermined attack scenario and therefore  probe his own probability of defence.
What makes this setting particularly well-suited to real-world security applications is in that, 
1. unlike existing work, no assumption is required to be made about the defender's knowledge of its utility matrix $A$. Instead, the utilities can be learned offline through probing the individual entries of $A$, and 
2. the stochastic framework gives a natural model for the players' uncertainty about the environment.

As discussed earlier, in light of the robustness required by real security schemes, 
it is necessary for the designed algorithms to not only be efficient but also possess theoretical guarantees. 
To this end, we will first analyse the true complexity of the problem in the setting described above, 
and then design an algorithm to best capture the obtained hardness.   
%The objective of this  approach is to determine  the best strategy during a given exploration phase and  is therefore  closely related to the general theory of optimisation and has been study in the discrete context of multi arm bandit as pure exploration problems \cite{Audibert10BA}. This initial work has been extended in a flurry variant setting where one tries to find the best(s) arms.
%Victor has a nice expertise in that and has participated to the extension and application of such a framework in more and more complex problem (cite my work?) and is working on extension to combinatorial bandits that would improve upon the seminal work by Chen.


% Taking into account the particular  structure of the problem will be necessary when dealing with Stackelberg equilibrium in security games. There the function to optimise is even more complex. What is the complexity here?
The hardness of the best arm identification problem in the stochastic setting can be interpreted as the total number of pulls required to discriminate the best arm(s) from the others. In a simple multi-arm bandit setting, this is defined as the sum of the complexity of each suboptimal option, where the complexity of a suboptimal option $i$ is inversely proportional to the gap $\Delta_i= \mu^*- \mu_i$ between the value of the best option $\mu^*$ and that of option $i$.
More precisely the complexity $H$ is  defined as $H = \sum_{k} \frac{1}{\Delta_k^2}.$

%
%\begin{equation*}
%
%\end{equation*}
%
Extensions of this notion have been designed for combinatorial bandits \cite{chen2014combinatorial}. The fellow is currently working on a improved version of the state-of-the-art result. The complexity of an arm defined in these combinatorial games is  more complex than in the simple multi-armed bandit problem as it involves combinatorial quantities. Similarly in a security games, we expect the complexities for the defender to depend also on the actions available to the attacker.
%In a combinatorial setting, the forecaster must make a recommendation that is of combinatorial structure
% The (combinatorial) decision set is $\C\subseteq 2^K$ is such that any decision $U \in \C$ is a set of arms $U\subseteq \K$ and its value is the sum of their values, $\mu_U = \sum_{i \in U} \mu_i$. The value gap between two decisions is denoted by $\Delta_{U,V} = \mu_U - \mu_V$ and $U^* = \arg\max_{U\in\C} \mu_U$ is the best decision 
% \begin{align*}
%\Delta^\odot_k = 
%\begin{cases} 
%\mu^* - \max\limits_{V\in\C: k\in V} \mu_V & \text{ if } k\notin V^* \\
%\mu^* - \max\limits_{V\in\C: k\notin V} \mu_V & \text{ if } k\in V^* \\
%\end{cases}
%\end{align*}

Another issue that we want to address within Objective 1) is the scalability of our algorithms. It is natural to think that one could make use of the natural combinatorial nature of the actions in security games (placing a limited number of checkpoints in a finite number of locations). Indeed enumerating all the actions as in the standard formulation of security games makes the computations rapidly intractable. Therefore bringing our expertise in combinational pure exploration bandits could prove useful. We moreover propose a simplifying first step to address even further the issue of scalability in  this combinatorial problem. Making use of some submodularity property that often arise in the context of sensor (or checkpoint) placement can provide tractable and almost optimal algorithms. Therefore submodular combinatorial pure exploration is a first step.

%One first step is to relax the problem as shown in Krause et al finding the best response to a given adversary. This is known to be is NP hard problem  but can be solve almost optimally be a greedy algorithm thank to a sub modularity property of the problem. This gives rise to a first objective which would be to learning optimise stochastic submodular function under a pure exploration setting.
%Note that I worked on similar subject with learning in submodular functions.


\textbf{Objective 2:  Repeated Network Security Games.}
Security problems often need to secure complex structures and networks. Taking advantage of the knowledge of these structure is of great importance to create efficient and computational tractable algorithms. In the case of graph structure, graph decomposition  tools are relevant. For this purpose a collaboration with Michal Valko , a word famous expert of active learning on graphs and part of the partner organisation Inria Lille, will be of great help.
 
%cite mumbai

A very interesting problem is the problem of (mobile) smuggler arrest in a network\cite{jain2011double} an problem that is often motivated in the literature as modelling a  response  to  the  Mumbai  attacks  of  2008, after which the  Mumbai  police
have started to schedule a limited number of inspection checkpoints
on the road network throughout the city. This problem has not been studied in its repeated form, where a pursuit-evasion games is played multiple times against a series of different smugglers. Therefore the strategy of the defender is \textit{not adaptive} to the previous observations collected about the previous attackers strategy and is therefore sub-optimal. This problem is a key fundamental problem modelling security games on a graph for which learning methods are necessary. Moreover it brings together two interesting works. First it can be seen as an instance of the work on  general adversarial combinatorial bandits\cite{cesa2012combinatorial}. This latter set of work considers a more general problem, therefore does not capture all the subtility of the specific graph pursuit-evasion problem: it does not restrict attacker to play a path in the graph. Second it is a generalisation of the work of Balcan\cite{Balcan15CR} to the case where the number of $k$ unknown matrix is extremely large but posses a combinatorial structure as each matrix correspond to a path in a graph.  
%The idea would be to  extend the work of Balcan using more complex bandit algorithms. They use a version with k known attackers. We can assume that k is extremely large but there is some  structure that permits us to use for instance combinatorial bandits.

%The security issue naturally has application in graph problem that model the network of roads/ connection between computers that agents might need to secure. Therefore there has been study that apply game theory to this problems. For instance it has been used to monitor road barrage in mumbai (connection) The goal is there to put some check point on a road to stop some terrorist. Its a one shot game where you try to minimise the probability of the player to pass.  Utilities are not really defined and complex here You just want to maximise the probability of catching the attacker. We are interested in a version of this game that is repeated . Everyday the same problem arises. We would minimise the cumulative regret. Therefore the defender can be adaptive and if the attacker is not smart and repeat always the same plan we will catch him often (not totally a worse case scenario). This can be seen actually has a specific problem of adversarial combinatorial bandits where the  attacker is limited to a very specific structure of losses which are path in a graph. We can expect to use the specificity of the graph by using some result from spectral graph theory. Maybe also we can use this theory to solve some issue with the scalability of the algorithm.

\textbf{Objective 3: Regret Analysis of Repeated Security Games.}
In some applications where a newly created security systems is both not provided with any  historical data and cannot be tested before being used in production, the learning must be performed online while actually playing the security game. In this context it is of high importance that the learning is done as fast as possible. This means that only providing an analysis that shows the asymptotic convergence is definitely not enough and  calls for algorithms that are provided with their finite time sample analysis. A natural quantity of interest is the  \textit{cumulative regret},  standard  in multi-armed bandits analysis. In this formulation, the objective for the forecaster is to minimize the expected cumulative regret after $n$ pulls $R(n)$ defined as
%
\begin{equation*}
R(n)=\max_i\sum^n_{t=1} l^t_{i}-\E\left[\sum_{t=1}^n l^t_{I(t)}\right].
\end{equation*}
%
For security games, the cumulative regret is the difference between the sum of rewards collected by always using the best security strategy in hindsight and the sum of rewards actually collected by the forecaster.
This is an online game where, at each time set $t$ of the game, a fundamental trade-off arises for the forecaster between the simultaneous need to select the security strategy solution he currently thinks is the best in order to maximise the immediate security given his current knowledge  (exploitation)  while also wanting to test possible other strategies that might or might not be better (exploration).

Our goal here will be to provide a regret analysis for the online stochastic repeated security games. The first step will be study  how to design a new algorithm borrowing ideas from the UCRL algorithm \cite{auer2009near} that was extending the bandits ideas of the classical UCB analysis to general reinforcement learning problems
For this project collaboration with  Bruno Scherrer, a fellow's coauthor, from Inria Nancy is desirable as he has recently analysis how reinforcement learning asymptotic analysis could be applied to zero sum games\cite{scherrerapproximate}.

%talk about the UCT stuff?

Finally an important other possible additional requirement is that we learn security strategies that are not only of good quality in average but also whose performance is not subject to large variance when used on a daily basis. This requirement has been well-studied in the statistical community has a \textit{risk-averse} requirement. Recently this requirement has been considered in a multi-armed bandit framework\cite{NIPS2012_4753}. An implementation in the security games specific context is therefore natural.


%A popular efficient  algorithm for it is Thompson Sampling as it has both proved very efficient\cite{Chapelle11EE}, handy and as recently started to be theoretically good~\mbox{\cite{Kaufmann12TS}}. 

%Note that a very numerous of variation of the initial games ranging from considering  infinite number of arms to continuous actions\cite{Wang08AI,Abbasi-Yadkori11IA,Dani07TP} permits these methods to adapt to a very large mount of challenges

\noindent \textbf{\textit{\\Originality and innovative aspects of the research programme:}}\\

 
\noindent \textbf{\textit{\\Timeliness and relevance:}}\\
Security games has found numerous applications in the recent 10 years and has proved that it could bring a significant improvement over  systems designed by humans. Making this systems even more autonomous so that they can autonomously improve by constantly learning is a key issue to bring low maintenance security systems. We believe machine learning is providing most of the necessary tool to reach this goal. Machine Learning is one of the most rapidly growing community in computer science research as it is currently making a difference in key industrial sectors already  building strong recommendation systems, and being used by the most important companies of our time to create self-driving cars or new pattern recognition algorithms.   The connection between security games and machine learning is evident, work that have connects those two worlds are still rare and it will be the source of many new research challenges. 
Finally bringing more robust security systems is of high importance for Europe has recent event have shown how ensuring the security of network communications from spy or securing public transportation is crucial.
 