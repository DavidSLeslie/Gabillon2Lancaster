
\subsection{Quality, innovative aspects and credibility of the research (including inter/multidisciplinary aspects)}
\label{sec:quality}


A critical concern in the modern world is security. Effectively protecting the ports, airports, trains and other transportation systems from malicious attacks, fighting the trafficking of drugs, and firearms, and securing proprietary and sensitive information over the ever-growing, modern cyber-networks comprise some of the principal axes of this critical task. The main challenge in all of these problems is that maximum security must be obtained with a limited number of available resources. For instance, the total number of security agents available to simultaneously protect a multitude of designated targets may not be sufficient to provide full security coverage at an airport. This calls for the design of appropriate resource allocation techniques which, given the available resources, would result in  maximum security coverage. %Clearly, an important feature of these methods must be to provide highly unpredictable strategies, while devising appropriate target priorities in the presence of uncertainty about the adversaries' interests.  



Security resource allocation and scheduling problems comprise one of the many application areas, that have recently been shown to greatly benefit from game-theoretic approaches. Indeed, as a solid mathematical framework to model strategic decision making, game theory has proved useful in many real-world applications from economics and political science to logic, computer science and psychology. In this paradigm, the problem is cast as a ``game'' and the objective is to find a solution whereby each ``player'' makes choices to maximise her own utilities, which may often be in conflict with those of her opponent. A ``security game'' corresponds to a competition between a defender and an attacker. As discussed further in the sequel, to solve a security game, all possible actions (attacks and defences) of the two players are enumerated, and for each player an outcome (value) is assigned, which depends on the pair of actions taken by both players. In cases where these outcomes are known, game-theoretic approaches have proved remarkably useful in providing maximum security. 

Since 2007, the so-called ARMOR software \cite{pita2008deployed} is used at the Los Angeles International Airport (LAX) to effectively determine checkpoints on the roadways leading to the airport, and to canine patrol routes within terminals. Similarly, such programs as IRIS \cite{tsai2009iris}, PROTECT \cite{shieh2012protect}, and TRUSTS \cite{yin2012trusts} are respectively being deployed at the US Federal Air Marshals, the US coast guard patrolling, and the Los Angeles Metro system's fare inspection strategy. 

In general, in most real-world scenarios the actual outcomes of the game are not available to be fed to the solver, and must be estimated via expert educated guess or data-driven approaches. However, the data from which these values can be obtained are usually scarce and the error in expert data may be high, rendering the security game solver useless. An example of this situation can occur when, one of the players cannot completely foresee the outcome of oneâ€™s action, or is bound to ignore what utilities the opponent is aiming to maximise. 

%For instance, the efficacy of check-points at an airport may be be uncertain and the routes that are the most profitable for the drug-smuggler might be unknown.


Machine learning is a field of artificial intelligence where the goal is to design software able to \textbf{ 1)} actively (and smartly) collect data in order to \textbf{2)}  extract information from this data so that \textbf{3)} the machine itself can make use of this information to take autonomous decisions.  Based on solid statistical theoretical background, machine learning has been applied in a very large variety of modern applications ranging from robotics to personalised product recommendation. 


Learning is possible in security games as this game is often played and repeated on a daily basis between the security services and the possible attackers. Therefore each day the security agency can collect more data about the parameters of the game. Another possibility is in fact that the security agency can itself test the game and collect (in the most efficient and less costly manner) more information about the game. The key objective forming the basis of this grant proposal, is thus to design efficient and theoretically sound, data-driven methods that can actively interact with the environment to {\em learn} a fair model through repeated games. Using machine learning, the goal will be either \textbf{1)} to limit the costs imposed by this extra need to learn the model or  \textbf{2)} to actually design an efficient data mining procedure before the game starts that guarantee a good defence strategy.



\noindent \textbf{\textit{\\State-of-the-art}}
\noindent \textbf{\textit{\\Security meets Game Theory }}\\
\TODO{better distinction between minimax nash stacjkelberg}
From a game-theoretic perspective, a security problem is viewed as a two-player game that captures the interaction between a defender (e.g., border patrols, metro inspectors, network administrators) and an attacker (e.g., terrorists/drug smugglers, illegal metro users, malicious cyber attackers). The action of the defender (attacker) is defined as selecting a subset of targets to protect (attack). For each defender/attacker action pair, utilities are defined as the players' gain or loss, and the players' objectives are to maximise their corresponding pay-offs. From the defender's perspective, this corresponds to efficiently allocating a limited number of resources to secure some predefined targets from the attacker. 
%The defender allocates a limited number of resources to secure some predefined target, which may under threat by the attacker. 
Solutions to such games rely on randomised strategies, making the defender's scheme highly unpredictable for the attacker, thus giving rise to a significant advantage over the original mechanisms that are based on deterministic human schedulers. In the case of games that are fully competitive between the two players  (i.e. the so-called zero-sum games), these methods are provably robust in that they provide guaranteed performance against {\em any} possible attacker. In this case, such guarantees hold, even if the defender's strategy is completely revealed to the attacker.  
The extension of this guarantee to a more general (non zero-sum) game is provided by Stackelberg equilibrium, a notion that generalises the famous Nash equilibrium \cite{korzhyk2011stackelberg}. 
%Beyond these fundamental results, 
%some questions forming the primary focus of research in security games have been scalability, robustness with respect to uncertainty on the players utilities, or devising strategies that take advantage of the attacker's potentially limited rationality or bounded memory\cite{tambe2012game}. 

\noindent \textbf{\textit{\\Sequential decision-making under uncertainty}}\\
Machine learning is a field of artificial intelligence where the goal is to design software able to extract information from data so that the machine itself can make use of this information to take  autonomous decisions.
The problem of sequential decision-making under uncertainty arises in everyday life, when we try to find an answer to questions like how to navigate from home to work, how to play and win a game (e.g.,~backgammon, poker, or the game of Tetris that has been used as an experimental testbed in Victor's thesis), how to retrieve our information of interest from the Internet, how to optimize the performance of a factory, etc. Such applications naturally involves uncertainty about the consequence of the player action (in the game of Tetris, the next falling piece is sampled randomly, in a security game the success of randomly selecting passengers to be searched is inerrantly unpredictable). To solve this problems machine learning has proposes methods and techniques that borrow and extend a lot from the fields of statistics (in order to take into account the uncertainty around the data) and optimisation (to create fast converging methods, minimising for instance the number of actions to get to specific level of performance). %It has proven to make a difference in practice and has nice mathematical background. We want to focus here on two particular methods that are interesting in machine learning and where Victor and David are experts.
%First, many interesting sequential decision-making tasks can be formulated as reinforcement learning (RL) problems. In RL, an agent interacts with a dynamic, stochastic, and incompletely known environment with the goal of learning a strategy or \textit{policy} to optimize some measure of its long-term performance (e.g.,~to remove as many lines as possible in Tetris).
%talk very quickly about MDP POMDP tree search?


A core framework for learning is the \textit{multi-armed bandit problem}. It is a game where at each time step the same fixed set of actions is available to the player. It can be formalized as a game between an environment and a forecaster. In its simplest form there are $K$ actions (arms). At each round $t$, the environment allocates rewards to each arm (described as a vector $l^t\in\R^k$) while simultaneously the forecaster chooses to pull an arm $I(t)$ and observes a reward $l^t_{I(t)}$.  The fundamental property is that the player does not get to observe the reward he would have collected if he had selected another arm instead. In a security context you could imagine that the K arms/actions are K possible security strategies whose value are following some random law and that can  only estimated by testing  them /sampling them /pulling the corresponding arm. For this proposal we focus on two different ways to measure the performance of interest of the methods.  The first formulation corresponds to the classical cumulative regret setting where the forecaster tries to constantly choose the security strategy with the highest value on average.  The second one is the ``pure exploration'' setting, where the forecaster can uses the exploration phase, a limited phase during which he can freely pull the arms he chooses to, in order to identify the best  security strategy among the $K$.

%Sequential decision-making tasks is often coupled with online learning in the sense such problem needs to learn online how to solve a task while solving it. These two features, sequential decision making and online learning while be most required for our new problems right! elaborate a bit

\TODO{Maybe add success of bandits.}

\textbullet~ The cumulative regret setting is the standard formulation for multi-armed bandits. In this formulation, the objective for the forecaster is to minimize the expected cumulative regret after $n$ pulls $R(n)$ defined as
%
\begin{equation*}
R(n)=\max_i\sum l_{i,t}-\E\left[\sum_{t=1}^n l_{I(t),t}\right].
\end{equation*}
%
Here a fundamental trade-off arises between the simultaneous need to select the solution we currently think is the best in order to maximise the immediate reward  (exploitation)  while also wanting to test possible other choice/actions that might or might not be better (exploration).

The problem has been studied under two main assumptions about the power of the environment. In a first setting  the environment chooses the rewards in a stochastic manner, following some predetermined (however unkwnon to the player) distributions to allocate the rewards to the arms). This setting permits is very handy to model noise in the data and will be useful in security games when the defender is uncertain about the stochastic outcome of one of its action. A popular efficient  algorithm for it is Thompson Sampling as it has both proved very efficient\cite{Chapelle11EE}, handy and as recently started to be theoretically good~\mbox{\cite{Kaufmann12TS}}. In the second setting, no stochastic assumptions are made and it is actually as if an adversary could arbitrarily choose and design all the rewards
\cite{Auer03NS}. This latter setting is therefore a harder one and will be useful when modelling a competition between our two players, the defender and the attacker.

Note that a very numerous of variation of the initial games ranging from considering  infinite number of arms to continuous actions\cite{Wang08AI,Abbasi-Yadkori11IA,Dani07TP} permits these methods to adapt to a very large mount of challenges

\textbullet~ The pure exploration is a relatively new setting, where the forecaster is only evaluated at the end of an exploration phase comprising a limited number of pulls. Contrary to the cumulative regret setting, the rewards collected before the end of the game are not taken into account.  We see a very interesting application of this setting to security games when the defender can test his defensive strategy before putting it in application in the real world.
The objective is a play between the probability of error and the samples (pulls) required. 
In the \textit{fixed confidence} setting (see e.g.,~\cite{Maron93HR,Even-Dar06AE}), the defender would typically try to minimize the number of test (and hence the cost) needed to achieve a fixed confidence on the quality of the returned best estimated defence strategy while in the \textit{fixed budget} setting (see e.g.,~\cite{Bubeck09PE,Audibert10BA}), the number of tests of the exploration phase is fixed and is known by the defender, and the objective is to maximize the probability of returning the best strategy at the end of the phase. 

%The problems put forward a notion of complexity of the problem. That we try to characterise well. And that we will try to study in our new problems.

\noindent \textbf{\textit{\\Frontiers of Security Games: From handling uncertainty towards self-learning algorithms}}\\
Some of the main issues forming the primary focus of research in security games have been scalability, the ability for the designed methods to handle problems with very large number of actions for the players, or devising strategies that take advantage of the attacker's potentially limited rationality or bounded memory\cite{tambe2012game}. Closer to our current interest, another important research goal that has been extensively addressed is to devise methods that are robust with respect to uncertainty about the environment\cite{aghassi2006robust,Nguyen14RO, Kiekintveld:2013}.
Granick, for example,
argues that weaknesses in our understanding of the measurability of losses serve as
an impediment in sentencing cybercrime offenders\cite{granick2005faking}. Swire adds that deterring
fraudsters and criminals online is hampered if we cannot correctly aggregate their
offences across different jurisdictions \cite{swire2009no}. These  interest in dealing with uncertainty in the data shows how important and crucial and costly it could be and that it is a real issue in real problems.


However, still little has been done to give a realistic active learning solution to the uncertainty about  the unknown environment. Achieving this goal is indeed crucial, since algorithms that make use of environmental knowledge are arguably more reliable than those merely designed to be robust against this lack of information. With this motivation, some interesting advancements have very recently been made through links with optimisation and machine learning methods. 
These methods focus mostly on the case where the attacker's preferences are not fully known and are thus to be learned; the learning objective is achieved through a repeated a game.  \cite{blum2014learning, letchford2009learning} propose analyses in terms of the number of required queries to learn the optimal defender's strategy. 
Marecki et. al. and Qian et. al.\cite{Marecki12PR, qian2014online} take an empirical Bayesian approach where, given a prior distribution, planning techniques based on Partially Observable Markov Decision Processes (POMDPs) are used to update the posterior over the adversary's preferences.
The main theoretical drawback of this planning method is in that the algorithm is based on Upper Confidence Trees (UCT), which, as shown by  \cite{munos2014bandits}, are provably sub-optimal. 
Recently an extended analysis is given by \cite{Balcan15CR}  for the case of multiple attackers, where at each round of the game, a single attacker is chosen adversarially from a fixed, finite, set of known attackers. The latter work shows strong connections with adversarial bandit theory. 

%\textbf{Motivation.} %%%V what can I bring


\noindent \textit{\textbf{Main Goal}}\\ The purpose of this proposal is to design new methods for security games that are even more practical  in the sense that they would be autonomous in handling the uncertainty in the model and would actively be working at reducing it by interacting with the environment in which the game takes place. We desire to broaden the scope of repeated security game problems where the initial uncertainty about the players utilities can be overcome through using learning techniques  in conjunction with  repetitive plays of the game. These techniques are from the booming field of machine learning.
First we will look at new instances of these problem that are related to our area of bandit expertise and that serve real purpose.

To solve real world problems our higher priority fields of investigations will be:

\begin{itemize}
\item \textit{Explore stochastic assumption:} Aside from contributing to adversarial setting, One of our interest  is to make Stochastic assumptions when dealing with noise in the model and adversarial assumption when dealing with the adversary to make our approach both realistic and robust.
%%%A depending on the considered scenraio, it may make sense to consider the problem as adversarial or stochastic. The distinction will be made clear in the ... 
So far stochastic noise in the model  in conjunction with learning has been untouched while it can happen when phenomenon are not inherently adversarial (sensor that work stochastically action that have stochastic outcomes, checkpoint might not stop deterministically the attacks and the probability of success needs to be determined, here learned).
\item \textit{Efficient learning algorithms:} 
Our goal is to have a theoretically sound approach by designing efficient algorithms for which we can provide finite sample analysis. Mention my previous work?

\item \textit{Scalable learning algorithms:} Extending the previous approaches to complex problem that involves some combinatorial structure is also important. Submodularity emtion work?
\item \textit{Different feedback structures:} The complexity of learning highly depends on the quality of the feedback that the learned can collect. From the most informative full information feedback to the many variations of partial feedback setting it is of importance, as discussed in Objective 1, to quantify how the nature of the feedback  affects the performance and to focus on scenario that correspond to real world example.
\item \textit{Robust learning:} When dealing with security game, robustness is a key issue. While the need for learning inherantly add some ecurity cost we want to look at instance of the learning problem that preserve some notion of robustness. The first way to be robust is to be very conservative and assume that an adversary actually chooses (in the most adversarial way) the data that we do not know
A first way to  reply to that is to use setting where no assumption use of \textit{adversarial} algorithm.
 Another possible concern that might happen in some problems pure best arm identification 
 Another possibility is that in some situation we do not want the learning process to happen during the use of the program but before hand. Then we can assume that we use of a pre launch exploration phase where we try to learned as precisely as possible the model given some budget constraint or some targeted performance guarantees. 
  
 An
this need for robustness needs to be mitigated depending on the application. some real world problem might need an extra care on robustness like terrorist attack while others are not that sensitive to it.
 In the latter case we should not be to conservative in the learning to be able to learn faster.

 we might be required to learn defence strategies that are not necessarily the best in expectation but instead also guarantee not to possess large variances in their performance. Here we plan to make connection with risk averse learning algorithm.
 \item \textit{Adaptivity ?}
\end{itemize}


\textbf{Objective 1 Pure exploration in Stackelberg games}
Bringing the pure exploration in bandits to Stacklerberg games is the natural first phase of our project. As it is first one of the main domain of expertise of Victor and two it brings a first conservative way to address learning without bringing to much risks for the learner. Indeed in this setting the learning of the unknown model happens during an exploration phase before putting on the market.  This for instance means that he can run tests of the security in a variety of predetermined attack scenario and therefore  probe his own probability of defence.

We would make the stochastic assumption as here it accounts for noise in our model and not adversary actions.
 The objective of this  approach is to determine  the best strategy during a given exploration phase and  is therefore  closely related to the general theory of optimisation and has been study in the discrete context of multi arm bandit as pure exploration problems \cite{Audibert10BA}. This initial work has been extended in a flurry variant setting where one tries to find the best(s) arms.
Victor has a nice expertise in that and has participated to the extension and application of such a framework in more and more complex problem (cite my work?) and is working on extension to combinatorial bandits that would improve upon the seminal work by Chen.


 Taking into account the particular  structure of the problem will be necessary when dealing with Stackelberg equilibrium in security games. There the function to optimise is even more complex. What is the complexity here?
 
 \paragraph{\textbullet$\;$  Complexity:} The hardness of the best arm identification problem in the stochastic setting can be interpreted as the total number of pulls required to discriminate the best arm(s) from the others. In simple multi-arm bandit setting it is defined as the sum of the complexity of each suboptimal options, where the complexity of a suboptimal option $i$ is invertionally proportional to the gap $\Delta_i= \mu^*- \mu_i$  the difference between the value of the the best option $\mu^*$ and the value of option $i$
 
 More precisely the complexity $H$ is  defined as
%
\begin{equation}
H = \sum_{k} \frac{1}{\Delta_k^2},
\end{equation}
%
Extensions of this complexity notion have been designed in more complex setting like combinatorial bandits cite{Chen}. Note that Victor is currently working on a improved version of this result. In a combinatorial setting, the forecaster must make a recommendation that is of combinatorial structure
 The (combinatorial) decision set is $\C\subseteq 2^K$ is such that any decision $U \in \C$ is a set of arms $U\subseteq \K$ and its value is the sum of their values, $\mu_U = \sum_{i \in U} \mu_i$. The value gap between two decisions is denoted by $\Delta_{U,V} = \mu_U - \mu_V$ and $U^* = \arg\max_{U\in\C} \mu_U$ is the best decision 
 \begin{align*}
\Delta^\odot_k = 
\begin{cases} 
\mu^* - \max\limits_{V\in\C: k\in V} \mu_V & \text{ if } k\notin V^* \\
\mu^* - \max\limits_{V\in\C: k\notin V} \mu_V & \text{ if } k\in V^* \\
\end{cases}
\end{align*}

In case of of games the picture would be even more complex as the complexity would depend on the actions that adverasry have available.

One first step is to relax the problem as shown in Krause et al finding the best response to a given adversary. This is known to be is NP hard problem  but can be solve almost optimally be a greedy algorithm thank to a sub modularity property of the problem. This gives rise to a first objective which would be to learning optimise stochastic submodular function under a pure exploration setting.
Note that I worked on similar subject with learning in submodular functions.

connections with risk averse (Cite the work of Amir Sani) maybe a separate section for this.
talk about the classical cumulative regret setting also!

this also ask the long term question of convergence when both player are learning at the same time their utilities.

\textbf{Objective 2 Learning  more complex adversarially chosen attacker in  Stakleberg}
The idea would be to  extend the work of Balcan using more complex bandit algorithms. They use a version with k known attackers. We can assume that k is extremely large but there is some  structure that permits us to use for instance combinatorial bandits.



\textbf{Objective 3 Repeated Network Security Games}
The security issue naturally has application in graph problem that model the network of roads/ connection between computers that agents might need to secure. Therefore there has been study that apply game theory to this problems. For instance it has been used to monitor road barrage in mumbai (connection) The goal is there to put some check point on a road to stop some terrorist. Its a one shot game where you try to minimise the probability of the player to pass.  Utilities are not really defined and complex here You just want to maximise the probability of catching the attacker. We are interested in a version of this game that is repeated . Everyday the same problem arises. We would minimise the cumulative regret. Therefore the defender can be adaptive and if the attacker is not smart and repeat always the same plan we will catch him often (not totally a worse case scenario). This can be seen actually has a specific problem of adversarial combinatorial bandits where the  attacker is limited to a very specific structure of losses which are path in a graph. We can expect to use the specificity of the graph by using some result from spectral graph theory. Maybe also we can use this theory to solve some issue with the scalability of the algorithm.



\noindent \textbf{\textit{\\Originality and innovative aspects of the research programme:}}\\

 
\noindent \textbf{\textit{\\timeliness and relevance:}}\\
Security is booming since 5 years
mahcine laerining also
the conjuntion of the two is definietly relevant but still largely unexplored
Europe wants security
migrants/ spy
 