
\subsection{Quality, innovative aspects and credibility of the research (including inter/multidisciplinary aspects)}
\label{sec:quality}

A critical concern in the modern world is security. Effectively protecting the ports, airports, trains and other transportation systems from malicious attacks, fighting the trafficking of drugs, firearms and even people, and securing proprietary and sensitive information over the ever-growing cyber-networks, comprise some of the principal axes of this critical task. The main challenge in all of these problems is that maximum security must be obtained with a limited number of available resources. For instance, the total number of security agents available to simultaneously protect a multitude of designated targets may not be sufficient to provide full security coverage at an airport. This calls for the design of appropriate resource allocation techniques which, given the available resources, would result in maximum security. %Clearly, an important feature of these methods must be to provide highly unpredictable strategies, while devising appropriate target priorities in the presence of uncertainty about the adversaries' interests.  
 
Security resource allocation and scheduling problems comprise one of the many application areas that have recently been shown to greatly benefit from game-theoretic approaches. Indeed, as a solid mathematical framework to model strategic decision making, game theory has proved useful in many real-world applications from economics and political science to logic, computer science and psychology. In this paradigm, the problem is cast as a ``game'' and the objective is to find a solution whereby each ``player'' makes choices to maximise her own \textit{utilities}, which may often be in conflict with those of her opponent. A ``security game'' corresponds to a competition between a defender and an attacker. To solve a security game, all possible actions (attacks and defences) of the two players are enumerated, and for each player an outcome (value) is assigned, which depends on the pair of actions taken by both players. In cases where these outcomes are known, game-theoretic approaches have provided impressive results. Since 2007, the so-called ARMOR software \cite{pita2008deployed} is used at the Los Angeles International Airport (LAX) to effectively determine checkpoints on the roadways leading to the airport, and to canine patrol routes within terminals. Similarly, such programs as IRIS \cite{tsai2009iris}, PROTECT \cite{shieh2012protect}, and TRUSTS \cite{yin2012trusts} are respectively being deployed at the US Federal Air Marshals, the US coast guard patrolling, and the Los Angeles Metro system's fare inspection strategy. 

A severe limitation of these models is that the true utility functions are usually unknown and must be estimated by experts or obtained from historical data. As a result, the potentially high estimation errors or the lack of historical data from which is suffering any newly established security system, will often render the security game solver useless. 
%However, the data from which these values can be obtained are usually scarce and the error in expert guesses may be high, rendering the security game solver useless. 
%A common example is when the players cannot completely foresee the outcome of some of her actions, or is bound to ignore what utilities her opponent is aiming to maximise. For instance, the exact efficacy of check-points or the drug smugglers' routes at an airport may be unknown. 
%Indeed, an important observation, forming the basis of this grant proposal, is that this {\em learning} problem lying at the heart of security games can be solved using carefully designed machine learning techniques. 
Therefore it is of importance to use methods that can quickly collect relevant data  in order to estimate the parameters of the game and quickly be operational.

Machine learning lies at the crossroad between statistics and computer science. The common goal is to design programs able to actively and intelligently gather data and extract information from the collected data, autonomously using them to make strategic decisions. Based on theoretically sound statistical methods, machine learning techniques are ubiquitously being deployed in a variety of modern applications ranging from robotics to personalised product recommendation. 

%\TODO{rewrite this end!!}

%The purpose of this proposal is to create \textit{practical}, \textit{scalable} and \textit{robust} methods for security games. First, we target \textit{practicality}  in the sense that our algorithms would be autonomous in handling the uncertainty in the model and would actively be working at reducing it by interacting with the environment in which the game takes place. Specifically, we aim to broaden the scope of repeated security game problems where the initial uncertainty about the players utilities can be overcome through using learning techniques  in conjunction with  repetitive plays of the game. 
%These techniques are from the extremely active field of machine learning research. Second, we target \textit{scalability} so that the solvers could handle a potentially  extremely large number of possible actions. To that end, simplifying structure assumptions such as combinatorial structure will be considered as well as the simplifying submodular property of the objective function.
%Finally the \textit{robustness}  is a key issue in security games. We propose to insure robustness in  three different ways. First, one can be  very conservative and assume that an adversary actually chooses (in the most adversarial way) the data that we do not know. Second, we introduce offline learning  where the test and the learning happen  before the strategies is actually use in the real world. Third, we might be required to learn defence strategies that do not possess large variances in their performance.

 {\em Learning} is a very well fitted approach to tackle security games as they are often of repeated form, played daily between a defender and possible attackers. Repeated security games allow for the continuous collection of data, which, through data-driven approaches can in turn be used to estimate the parameters of the game. Another possibility is in fact that the defender can in turn test the game and collect (in the most efficient and less costly manner) more information about the game. The key objective forming the basis of this grant proposal, is thus to design efficient and theoretically sound, data-driven methods that can actively interact with the environment to {\em learn} a fair model through repeated games. Using machine learning, the purpose of this proposal is to create \textit{practical}, \textit{scalable} and \textit{robust} methods for security games. 

 

 

\noindent \textbf{\textit{\\State-of-the-art}}
\noindent \textbf{\textit{\\i. Security meets Game Theory }}\\
%\TODO{better distinction between minimax nash stacjkelberg}
From a game-theoretic perspective, a security problem is viewed as a two-player game that captures the interaction between a defender (e.g., border patrols, metro inspectors, network administrators) and an attacker (e.g., terrorists/drug smugglers, illegal metro users, malicious cyber attackers). The action of the defender (attacker) is defined as selecting a subset of targets to protect (attack). For each defender/attacker action pair, \textit{utilities} are defined as the players' gain or loss, and the players' objectives are to maximise their corresponding pay-offs. From the defender's perspective, this corresponds to efficiently allocating a limited number of resources to secure some predefined targets. These utilities for both players can be stored in two  matrices,  $\boldsymbol A$ for the defender,  $\boldsymbol B$ for the attacker, where $\boldsymbol A_{i,j}$ and  $\boldsymbol  B_{i,j}$  are the utilities for the defender, respectively the attacker, when the defender plays her strategy $i$ and the attacker responds with her $j^{\text{th}}$ strategy.
%The defender allocates a limited number of resources to secure some predefined target, which may under threat by the attacker. 
Solutions to such games rely on randomised strategies, making the defender's scheme highly unpredictable for the attacker, thus giving rise to a significant advantage over the original mechanisms that are based on deterministic human schedulers. 
Another important feature of this paradigm is that it allows to obtain theoretical guarantees. Specifically, in the case of games that are fully competitive between the two players  (i.e. the so-called zero-sum games), the solution is provably robust in that it provides guaranteed performance against {\em any} possible attacker. Moreover, such guarantees hold, even if the defender's strategy is completely revealed to the attacker.  
An extension of this guarantee to a more general (non zero-sum) game can be provided by Stackelberg equilibrium, a notion that generalises the famous Nash equilibrium \cite{korzhyk2011stackelberg}. Indeed, this is well-suited to the scenario where the defender's strategies may be at risk of revelation, leak or discovery through repetitive interactions. 
%Beyond these fundamental results, 
%some questions forming the primarouy focus of research in security games have been scalability, robustness with respect to uncertainty on the players utilities, or devising strategies that take advantage of the attacker's potentially limited rationality or bounded memory\cite{tambe2012game}. 

%\noindent \textbf{\textit{\\Tools from Sequential decision-making under uncertainty}}\\

\noindent \textbf{\textit{\\ii. Uncertainty in Security Games}}\\
Uncertainty is endemic in most real-world applications. 
Unlike standard game-theoretical approaches, in some scenarios, the players is uncertain about their utilities (the $A_{i,j}$'s), or those of the other players'. For instance, in the context of security games, the random selection of passengers for security checks at an airport is a source of uncertainty in this game, where the outcome is random and the probability of successful security enforcement  is unknown. 
As also confirmed by several empirical studies in fraud and cybercrime detection, this phenomenon can significantly decrease the defender's performance\cite{granick2005faking, swire2009no}.
Extensive studies have been dedicated to the design of security games that are robust with respect to uncertainty about the environment\cite{aghassi2006robust,Nguyen14RO, Kiekintveld:2013}. 
However, an important observation is that much more can be done in the case of {\em repeated} security games.  
Indeed, this repetition allows the defender to further reduce her uncertainty about the model and intelligently {\em learn} how to improve her performance over time. Specifically, in this case, a security game solver can autonomously take intelligent decisions at repeated instances of the game, by carefully collecting, extracting and acting upon information from historical data. As discussed further in the subsequent section, this is precisely the scope of machine learning tools. 

\noindent \textbf{\textit{\\iii. Indispensable Tools from Machine Learning}}\\
%Machine learning is a field of computer science where the goal is to design software able to extract information from data so that the machine itself can make use of this information to take  autonomous decisions.
%The problem of sequential decision-making arises in our everyday lives, when we seek answers to such simple questions as how to navigate to work, retrieve our desired information from the Internet, or even
%play and win at backgammon, poker or tetris (the latter was used as a testbed in the author's PhD thesis). Naturally, these situations incur uncertainty since not only are the dynamics of the environment stochastic but also because the underlying stochastic mechanism that generates these dynamics are unknown.   
%
%In the context of security games, the random selection of passengers for security checks at an airport is an example source of uncertainty in this game where the outcome is random and the probability of successful security enforcement  is unknown. 
%In these applications, a security game solver is required to autonomously take intelligent decisions at repeated instances of the game, and must thus carefully gather, extract and act upon information from historical data. 
%
Machine learning consists of data-driven techniques with strong ties to the fields of statistics (allowing them to account for uncertainty about the data) and optimisation (so as to quickly converge to the desired solution, minimising for instance, the number of actions required to achieve a specific level of performance).   	

%It has proven to make a difference in practice and has nice mathematical background. We want to focus here on two particular methods that are interesting in machine learning and where Victor and David are experts.
%First, many interesting sequential decision-making tasks can be formulated as reinforcement learning (RL) problems. In RL, an agent interacts with a dynamic, stochastic, and incompletely known environment with the goal of learning a strategy or \textit{policy} to optimize some measure of its long-term performance (e.g.,~to remove as many lines as possible in Tetris).
%talk very quickly about MDP POMDP tree search?


One of the fundamental problems in machine learning, relevant to our research objectives in this proposal, is the \textit{multi-armed bandit problem}. It corresponds to a scenario where the learner is required to actively collect data from an environment in order to solve a given task. The solutions built for this problem have found many practical applications from adaptive routing in a network to medical trials of new medicines\cite{bubeck2012regret}.  
The bandit problem is a repeated game where, at each time step the same (fixed) set of actions is available to the learner.  
In its simplest form there are $K$ actions (arms). At each round $t$, the environment allocates rewards to each arm (described as a vector $l^t\in\R^k$) while the learner simultaneously chooses an arm $I(t)$ to pull in order to obtain a reward $l^t_{I(t)}$. An important constraint in this setting is that the player is not allowed to observe the hypothetical reward that would have been collected had another arm been selected instead. This therefore corresponds to a simplified security games where the strategies of the other players are fixed and only an external environment allocates the rewards. One can think of a game where the $K$ arms/actions correspond to the $K$ possible security strategies whose values are determined according to some unknown underlying probability distribution. 
The average per action reward can be estimated through sampling, i.e. via pulling the corresponding arm. 
%The problem has been studied under two main assumptions about the rewards. 
  
%all of the utilities are chosen by an  the defender takes actions against an attacker who adversarially designs the rewards of the game.
%For this proposal we focus on two different ways to measure the performance of the methods that corresponds to two different security requirements. % The first formulation corresponds to the classical cumulative regret setting where the forecaster tries to constantly choose the security strategy with the highest value on average.  The second one is the ``pure exploration'' setting, where the forecaster can uses the exploration phase, a limited phase during which he can freely pull the arms he chooses to, in order to identify the best  security strategy among the $K$.

%Sequential decision-making tasks is often coupled with online learning in the sense such problem needs to learn online how to solve a task while solving it. These two features, sequential decision making and online learning while be most required for our new problems right! elaborate a bit


%The problems put forward a notion of complexity of the problem. That we try to characterise well. And that we will try to study in our new problems.

While the multi-arm bandit games are extremely indispensable to active learning, an important question is how they can be used in general security games where one has to also take into account the actions of the other players.


%\noindent \textbf{\textit{\\Frontiers of Security Games: From handling uncertainty towards self-learning algorithms}}\\
\noindent \textbf{\textit{\\iv. Existing results}}\\
%An extensively litterature has tried to  devise security games methods that are robust with respect to uncertainty about the environment\cite{aghassi2006robust,Nguyen14RO, Kiekintveld:2013}.
%Granick, for example,
%argues that weaknesses in our understanding of the measurability of losses serve as
%an impediment in sentencing cybercrime offenders\cite{granick2005faking}. Swire adds that deterring
%fraudsters and criminals online is hampered if we cannot correctly aggregate their
%offences across different jurisdictions \cite{swire2009no}. These  interest in dealing with uncertainty in the data shows how important and crucial and costly it could be and that it is a real issue in real problems.
Some interesting advancements in security games have recently been  made through links with optimisation and machine learning methods. 
These methods mostly focus on the case where the attacker's preferences are not fully known and are thus to be learned; the learning objective is achieved through a repeated a game. Some recent work analyse the number of required queries to learn the optimal defender's strategy \cite{blum2014learning, letchford2009learning}. 
Marecki et. al. and Qian et. al.\cite{Marecki12PR, qian2014online} take an empirical Bayesian approach where, given a prior distribution, planning techniques based on Partially Observable Markov Decision Processes (POMDPs) are used to update the posterior over the adversary's preferences.
The main theoretical drawback of this planning method is in that the algorithm is based on Upper Confidence Trees (UCT), which, are provably sub-optimal\cite{munos2014bandits}. 
Recently, an extended analysis has been given\cite{Balcan15CR} for the case of multiple attackers, where at each round of the game, a single attacker is chosen adversarially from a fixed, finite, set of known attackers. This corresponds to a case where the utility matrix $\boldsymbol B$ is chosen adversarially from a set of $k$ known matrices.  The latter work shows strong connections with adversarial bandit theory. 

%\textbf{Motivation.} %%%V what can I bring


\noindent \textit{\textbf{\\Main Goal}}\\
%The main issues that have been considered so far in the literature are scalability, the ability for the designed methods to handle problems with very large number of actions for the players, or devising strategies that take advantage of the attacker's potentially limited rationality or bounded memory\cite{tambe2012game}.
The purpose of this proposal is to create \textit{practical}, \textit{scalable} and \textit{robust} methods for security games. First, we target \textit{practicality}  in the sense that our algorithms would be autonomous in handling the uncertainty in the model and would actively be working at reducing it by interacting with the environment in which the game takes place. Specifically, we aim to broaden the scope of repeated security game problems where the initial uncertainty about the players utilities can be overcome through using learning techniques  in conjunction with  repetitive plays of the game. 
These techniques are from the extremely active field of machine learning research. Second, we target \textit{scalability} so that the solvers could handle a potentially  extremely large number of possible actions. To that end, simplifying structure assumptions such as combinatorial structure will be considered as well as the simplifying submodular property of the objective function.
Finally the \textit{robustness}  is a key issue in security games. We propose to insure robustness in  three different ways. First, one can be  very conservative and assume that an adversary actually chooses (in the most adversarial way) the data that we do not know. Second, we introduce offline learning  where the test and the learning happen  before the strategies is actually use in the real world. Third, we might be required to learn defence strategies that do not possess large variances in their performance.

%\textit{Efficient learning algorithms:} 
Our goal is to have a theoretically sound approach by designing efficient algorithms for which we can provide finite sample analysis.%First we will look at new instances of these problem that are related to our area of bandit expertise and that serve real purpose.
%To solve real world problems our \textit{long-term} goals are:

%\begin{itemize}
%\item \textit{Explore stochastic and adversarial assumptions:} In a first setting, the environment chooses the rewards stochastically, based on some predetermined (however unknown) distributions. In security games this stochastic assumption helps model the unknown dynamics of the problem which both the defender and attacker can learn through interaction with the environment. In the second setting, no stochastic assumptions are made. Instead, an adversary is assumed to have arbitrarily chosen and designed the rewards
%\cite{Auer03NS}. This can be used to model a more pessimistic security game where the utilities are chosen adversarially by the attacker.
%Aside from contributing to adversarial setting, one of our interest  is to make Stochastic assumptions when dealing with noise in the model and adversarial assumption when dealing with the adversary to make our approach both realistic and robust.
%%%A depending on the considered scenraio, it may make sense to consider the problem as adversarial or stochastic. The distinction will be made clear in the ... 
%So far stochastic noise in the model  in conjunction with learning has been untouched while it can happen when phenomenon are not inherently adversarial (sensor that work stochastically action that have stochastic outcomes, checkpoint might not stop deterministically the attacks and the probability of success needs to be determined, here learned).
. %Mention my previous work?




%The \textit{short-term} plan in achieving these goals is outlined in three main objectives below.




\textbf{Objective 1: Scalability in Pure exploration Bandits via Submodularity.}
As discussed previously bandit problems come very handy to model repeated games under uncertainty. Therefore addressing the question of scalability in repeated security games must be first addressed in the context of bandits. In security games, it is natural to make use of the combinatorial nature of the actions, placing a limited number of checkpoints in a finite number of locations. We believe that combinational  bandit techniques, which also form part of the fellow's area of expertise, will be key in solving this problem. However, naively enumerating all possible actions as in the standard formulation of security games makes the computations rapidly intractable.  A key observation is that in many cases the players' performance evaluation function is submodular. This particularly arises in the context of maximal coverage problem for sensor (or checkpoint) placement\cite{krause2011randomized}. 
 This submodularity property can in turn be used to provide tractable and almost optimal algorithms. The fellow provided work on addressing submodular maximization techniques in a bandit setting\cite{gabillon2013adaptive} under a regret analysis.  As discussed further in the objective 2, we are also interested the bandit formulation called pure exploration that will provide a safer scenario to tests the security strategies. As this bandit setting is part of the fellow expertise a very natural first step will be to consider combinatorial pure exploration under submodularity.
 
%  A numerous literature have been written on extending bandit approach to linear, combinatorial or continuous spaces. In this proposal we wil be more intereted in combinatorial assumption that aris ein security games  (games in networks, combinatorial actions). 
%HoweverRecently the fellow has proposed
 
 
\textbf{Objective 2: Pure Exploration in Security Games.}
%Contrary to the cumulative regret setting, the rewards collected before the end of the game are not taken into account. 
As discussed previously, the main challenge in applying security games to real-world scenarios is that the players' utility matrices $\boldsymbol A$ and $\boldsymbol B$ are unknown and must be learned. The learning strategy depends on the particular setting of the problem. In the first objective we consider the case where the defender is in fact able to safely examine her defensive strategies before applying them online. A familiar real-world example is the security system at an airport, which can be tested many times before being deployed as the principal defence scheme.

We observe that in this formulation of the problem, we can capture the learning module of the security game by a recent bandit setting called \textit{pure exploration}, where the learner is only evaluated at the end of an exploration phase comprised of a limited number of interactions with the environment. Pure exploration has generally proved useful in many practical scenarios. Specifically, its application to parallel action selection in robotic planning has been extensively studied by the fellow \cite{Gabillon11MB}. 
However, applying this approach to security games is an open research path with a strong potential to bring new and interesting angles to the domain. 
In this formulation, we assume both utility matrices $\boldsymbol A$ and $\boldsymbol B$ completely unknown. During the test phase, at every  mock repetition of the game, the defender is able to probe an entry $\boldsymbol A_{i,j}$ of its utility matrix $\boldsymbol A$, corresponding to the outcome obtained when the attacker plays her strategy $j$ and the defender responds with her $i^{\text{th}}$ strategy. The value obtained is {\em a noisy version} of the true entry $\boldsymbol A_{i,j}$. What makes this setting particularly well-suited to real-world security applications is in that, 
1. unlike existing work, no assumption is required to be made about the defender's knowledge of its utility matrix $\boldsymbol A$. Instead, the utilities can be learned offline through probing the individual entries of $\boldsymbol A$, and 
2. the stochastic framework gives a natural model for the players' uncertainty about the environment.
%
 
% \textbf{Complexity.}
 We aim to design a strategy for the defender to, either minimise the number of tests needed to identify an excellent strategy with a given level of confidence or to maximise her probability of identifying the best strategy given a fixed number of tests. In the classical multi-armed bandit problem, these optimisation objectives respectively correspond to a \textit{fixed confidence} setting\cite{Maron93HR,Even-Dar06AE} and a \textit{fixed budget} constraint \cite{Bubeck09PE,Audibert10BA}. In order to extend these  classical results to the setting proposed above, we will first carefully characterise the data-dependent complexity (hardnesss) of the problem  and then move on to designing algorithms to best capture the obtained hardness. In standard bandit, the complexity of an arm is inversely proportional to the gap $\Delta_i= \mu^*- \mu_i$ between the value of the best option $\mu^*$ and that of option $i$. Extensions of this notion have been designed for combinatorial bandits \cite{chen2014combinatorial}. The fellow is currently working on a improved version of the state-of-the-art result. The complexity of an arm defined in these combinatorial games is  more complex than in the simple multi-armed bandit problem as it involves combinatorial quantities. Similarly in security games, we expect the complexities of each entry of the matrix for the defender to depend also on the actions available to the attacker.
  Therefore we will study this problem by gradually increasing its difficulty as we will explore different partial feedback structures. First we note that a recent work\cite{goldberg2014query} on query complexity, corresponds to the simpler deterministic  version of this problem where it is assumed that the probing outcome corresponds to the {\em true value} of $\boldsymbol A_{i,j}$. Thus they do not analyse the number of queries required to correctly estimate each entry of the utility matrix, which in turn depends on the performance of the strategies that would use this entry. Therefore our first approach will be to combine ideas from pure exploration and the deterministic query complexity setting in a context where the defender can individually sample from any entry of the matrix. A second more challenging setting will be to consider the more adversarial learning problem where the defender chooses a strategy and only observes the value of the game when the attacker best-responds to the defender strategy.
 %Note that the hardness of the best arm identification problem in the stochastic setting can be interpreted as the total number of pulls required to discriminate the best arm(s) from the others. In a simple multi-arm bandit setting, this is defined as the sum of the complexity of each suboptimal option, where the complexity of a suboptimal option $i$ is inversely proportional to the gap $\Delta_i= \mu^*- \mu_i$ between the value of the best option $\mu^*$ and that of option $i$.
%More precisely the complexity $H$ is  defined as $H = \sum_{k} \frac{1}{\Delta_k^2}.$ 


%\textit{Different feedback structures:} The complexity of learning highly depends on the quality of the feedback that the learned can collect. From the most informative full information feedback to the many variations of partial feedback setting it is of importance, as discussed in Objective 1 below, to quantify how the nature of the feedback  affects the performance and to focus on scenario that correspond to real world examples.

 %
 %submodularity property of the problem that often arises in the context of sensors (or checkpoints) placement can provide tractable and almost optimal algorithms. Therefore submodular combinatorial pure exploration is a first step.



%One first step is to relax the problem as shown in Krause et al finding the best response to a given adversary. This is known to be is NP hard problem  but can be solve almost optimally be a greedy algorithm thank to a sub modularity property of the problem. This gives rise to a first objective which would be to learning optimise stochastic submodular function under a pure exploration setting.
%Note that I worked on similar subject with learning in submodular functions.



%The idea would be to  extend the work of Balcan using more complex bandit algorithms. They use a version with k known attackers. We can assume that k is extremely large but there is some  structure that permits us to use for instance combinatorial bandits.

%The security issue naturally has application in graph problem that model the network of roads/ connection between computers that agents might need to secure. Therefore there has been study that apply game theory to this problems. For instance it has been used to monitor road barrage in mumbai (connection) The goal is there to put some check point on a road to stop some terrorist. Its a one shot game where you try to minimise the probability of the player to pass.  Utilities are not really defined and complex here You just want to maximise the probability of catching the attacker. We are interested in a version of this game that is repeated . Everyday the same problem arises. We would minimise the cumulative regret. Therefore the defender can be adaptive and if the attacker is not smart and repeat always the same plan we will catch him often (not totally a worse case scenario). This can be seen actually has a specific problem of adversarial combinatorial bandits where the  attacker is limited to a very specific structure of losses which are path in a graph. We can expect to use the specificity of the graph by using some result from spectral graph theory. Maybe also we can use this theory to solve some issue with the scalability of the algorithm.

\textbf{Objective 3: Regret Analysis of Repeated Security Games.}
In some applications a newly created security system is not provided with any historical data and cannot be tested before being used in production. Here, the learning of the utilities must be performed online while actually playing the security game. In this context it is of high importance for the agent to learn the utilities as fast as possible. This means that only providing an analysis to demonstrate asymptotic convergence is definitely not enough. To address repetitive learning in security games we will here assume that the utility matrix $\boldsymbol  A$ is unknown to  defender and that, at each repetition of the game the defender will best respond to his current strategy. Therefore we are considering a setting that is related to the analysis of Stackelberg equilibrium. A natural quantity of interest is the \textit{cumulative regret},  which is standard  in multi-armed bandit analysis. In our context, the objective is or the learner-defender build a series of defence strategies $\pi_t$ for $t=1,\ldots, n$ to minimize the expected cumulative regret after $n$ pulls $R(n)$ defined as
% 
\begin{equation*}
R(n)=nv(G)-\sum_{t=1}^n \pi_t A b(\pi_t).
\end{equation*}
%
where the v(G) is the value of the game when the two players play according to the Stackelberg equilibrium of the game. The cumulative regret would be therefore defined the difference between the sum of rewards collected by always using the \textit{best security strategy} in hindsight and the sum of rewards actually collected by the forecaster.
This is an online game where, at each time set $t$ of the game, a fundamental trade-off arises for the forecaster between the simultaneous need to select the security strategy solution he currently thinks is the best in order to maximise the immediate security given his current knowledge  (exploitation)  while also wanting to test possible other strategies that might or might not be better (exploration).

Our goal here will be to provide a regret analysis for the online stochastic repeated security games. The first step will be to study  how to design a new algorithms borrowing ideas from the UCRL algorithm \cite{auer2009near} that was extending the bandits ideas of the classical UCB analysis to general reinforcement learning problems. As most of the approach in online learning, UCB implements a strategy that is optimistic in face of uncertainty, playing any strategy that could be the best given the level of uncertainty. It would be therefore very interesting to see if this optimism principle is still optimal in a case of an adversarial game especially in a case where the adversary knows the true utilities of the games and moreover knows your levels of uncertainty.
For this project collaboration with  Bruno Scherrer, a fellow's co-author, working at  Inria Nancy, would prove fruitful as he has recently analysis how reinforcement learning asymptotic analysis could be applied to zero sum games\cite{scherrerapproximate}.
Approaches bases on Thompson sampling\cite{russo2014information} will be also considered as they have proved very efficient in practice and correspond to the area of expertise of Prof. Leslie.
%talk about the UCT stuff?

Finally an important other possible additional requirement is that we learn security strategies that are not only of good quality in average but also whose performance is not subject to large variance when used on a daily basis. This requirement has been well-studied in the statistical community has a \textit{risk-averse} requirement. Recently this requirement has been considered in a multi-armed bandit framework\cite{NIPS2012_4753}. An implementation in the security games specific context is therefore natural.




\textbf{Objective 4:  Learning Stackelberg Equilibrium against Combinatorial Adversaries.}
Objective 4 will be devoted to solving security games with more complex action structures. 
Real-world security problems often involve large, complex networks. This includes, for instance, complex routes, or computer/communication networks. 
Indeed, taking advantage of the inherent structure of the problem is of great importance which can significantly help create efficient and computationally tractable algorithms. 
We propose to extend the recent work of Balcan\cite{Balcan15CR} that proposes to learn Stackelberg equilibrium strategies against $k$ unknown adversaries, to the case where the set of adversaries is of combinatorial nature. To this end a link with the classical work of combinatorial adversarial bandit is relevant\cite{cesa2012combinatorial}. Note that the fundamental problem of combinatorial bandits has never been considered in the the context of a Stakelberg games.
The issue of scalability will be addressed in light of the results found in Objective 1.

\textbf{Objective 5:  Repeated Network-Security Games.}
As a more concrete application of Objective 4, this objective will focus on the particular combinatorial structure that is a graph as this stucture is present in  numerous real-word applications.
In light of the ever-growing, modern, social and communication networks, an extremely important application area is that of (mobile) smuggler arrest in a network\cite{jain2011double}. This has received significant attention in the community, especially in  response  to  the  Mumbai  attacks  of  2008, after which  Mumbai  Police
started to schedule a limited number of inspection checkpoints
on the road throughout the city. This problem has not been studied in its repeated form, where a pursuit-evasion game is played multiple times against a series of different smugglers. Therefore, the strategy of the defender is \textit{not adaptive} to the previous observations collected about the attackers' historical strategy and is therefore sub-optimal. Since this simple problem possesses a graph structure, it can be effectively used as a preliminary step to test security games on a graph for which learning methods are necessary. Moreover, this setting can be viewed as an instance of the work on general adversarial combinatorial bandits\cite{cesa2012combinatorial}. Note however that the latter setting does not capture all the structure specific to the graph pursuit-evasion problem. More specifically, it does not restrict the attacker to play according to a {\em path} on the graph. Therefore the objective  is to design specific algorithms in cases where the problem it is possible to take advantage of specific graphical structure of the problem. We start by defining a notion that captures the hardness of the task depending on characteristics of the graph that we would discover. Note that, although the goal is to generate algorithms for security games on graphs, the results to be obtained will be expected to lay grounds for research in a more general setting of active learning with graph structure. The fellow is currently working to establish a collaboration with Dr Michal Valko, a world-famous expert in active learning on graphs and part of Inria Lille in France. This collaboration will be greatly beneficial not only in achieving the second objective of this proposal, but also to strengthen international links between Lancaster University in the UK and Inria research organisation in France.  
 
 
 %A popular efficient  algorithm for it is Thompson Sampling as it has both proved very efficient\cite{Chapelle11EE}, handy and as recently started to be theoretically good~\mbox{\cite{Kaufmann12TS}}. 

%Note that a very numerous of variation of the initial games ranging from considering  infinite number of arms to continuous actions\cite{Wang08AI,Abbasi-Yadkori11IA,Dani07TP} permits these methods to adapt to a very large mount of challenges

\noindent \textbf{\textit{\\Originality and innovative aspects of the research programme:}}\\
This grant proposal is original has it proposes to develop a bridge between two areas of research (Security games and Machine learning) that have both proved their practical capacities but have only rarely been used jointly. By addressing some of the fundamental challenges of this connection we expect to bring more attention to the potential of this connection and bring the two communities to collaborate more in order to design software that solve important security problems.
Our approach is to design algorithms are both theoretically grounded and of practical use in real world problems.
Moreover by collaborating with with the Security Lancaster Departement in the University of Lancaster, we hope to apply and test our methods to real world problems.
 
\noindent \textbf{\textit{\\Timeliness and relevance:}}\\
Security games has found numerous applications in the recent 10 years and has proved that it could bring a significant improvement over  systems designed by humans. Making this systems even more autonomous so that they can autonomously improve by constantly learning is a key issue to bring low maintenance security systems. We believe machine learning is providing most of the necessary tool to reach this goal. Machine Learning is one of the most rapidly growing community in computer science research as it is currently making a difference in key industrial sectors already  building strong recommendation systems, and being used by the most important companies of our time to create self-driving cars or new pattern recognition algorithms.   The connection between security games and machine learning is evident, work that have connects those two worlds are still rare and it will be the source of many new research challenges. 
Finally bringing more robust security systems is of high importance for Europe has recent event have shown how ensuring the security of network communications from spy or securing public transportation is crucial.
 