
\subsection{Quality, innovative aspects and credibility of the research} % (including inter/multidisciplinary aspects)}
\label{sec:quality}

A critical concern in the modern world is security. Effectively protecting ports, airports, trains and other transportation systems from malicious attacks, combating the trafficking of drugs, firearms and even people, and securing proprietary and sensitive information over the ever-growing cyber-networks, comprise some of the principal axes of this critical task. The main challenge in all of these problems is that maximum security must be obtained with a limited number of available resources. For instance, the total number of security agents is typically less than the number of targets that need to be protected. %available to simultaneously protect a multitude of designated targets may not be sufficient to provide full security coverage at an airport. 
This calls for the design of appropriate resource allocation techniques which optimise security under the constrained resources available, in the presence of uncertainty about the adversaries' interests. %Clearly, an important feature of these methods must be to provide highly unpredictable strategies, while devising appropriate target priorities in the presence of uncertainty about the adversaries' interests.  
 
Security resource allocation and scheduling problems comprise one of the many application areas that have recently been shown to greatly benefit from game-theoretic approaches. Indeed, as a solid mathematical framework to model strategic decision making, game theory has proved useful in many real-world applications from economics and political science to logic, computer science and psychology. In this paradigm, the problem is cast as a ``game'' and the objective is to find a solution whereby each ``player'' makes choices to maximise her own \textit{utilities}, which may often be in conflict with those of her opponent. A ``security game'' corresponds to a competition between a defender and an attacker. To solve a security game, all possible actions (attacks and defences) of the two players are enumerated, and for each player an outcome (value) is assigned, which depends on the pair of actions taken by both players. In cases where these outcomes are known, game-theoretic approaches have provided impressive results. Since 2007, the so-called ARMOR software \cite{pita2008deployed} has been used at Los Angeles International Airport to effectively determine checkpoints on roadways leading to the airport, and to determine canine patrol routes within terminals. Similar deployments have been made by the US Federal Air Marshals\cite{tsai2009iris}, the US coast guard\cite{shieh2012protect}, and to design the Los Angeles Metro system's fare inspection strategy\cite{yin2012trusts}. 

A severe limitation of these models is that they assume the utility functions to be known, whereas they must actually be estimated by experts or obtained from historical data. As a result, potentially high estimation errors or a lack of historical data (which is inevitable in a quickly-evolving security scenario) may render the security game solver useless. 
%However, the data from which these values can be obtained are usually scarce and the error in expert guesses may be high, rendering the security game solver useless. 
%A common example is when the players cannot completely foresee the outcome of some of her actions, or is bound to ignore what utilities her opponent is aiming to maximise. For instance, the exact efficacy of check-points or the drug smugglers' routes at an airport may be unknown. 
%Indeed, an important observation, forming the basis of this grant proposal, is that this {\em learning} problem lying at the heart of security games can be solved using carefully designed machine learning techniques. 
Therefore it is of importance to use methods that can quickly collect the most relevant data  in order to estimate the parameters of the game and quickly reach a satisfactory operational performance.

%Machine learning lies at the crossroad between statistics and computer science. The common goal is to design programs able to actively and intelligently gather data and extract information from the collected data, autonomously using them to make strategic decisions. Based on theoretically sound statistical methods, machine learning techniques are ubiquitously being deployed in a variety of modern applications ranging from robotics to personalised product recommendation. 

%\TODO{rewrite this end!!}

%The purpose of this proposal is to create \textit{practical}, \textit{scalable} and \textit{robust} methods for security games. First, we target \textit{practicality}  in the sense that our algorithms would be autonomous in handling the uncertainty in the model and would actively be working at reducing it by interacting with the environment in which the game takes place. Specifically, we aim to broaden the scope of repeated security game problems where the initial uncertainty about the players utilities can be overcome through using learning techniques  in conjunction with  repetitive plays of the game. 
%These techniques are from the extremely active field of machine learning research. Second, we target \textit{scalability} so that the solvers could handle a potentially  extremely large number of possible actions. To that end, simplifying structure assumptions such as combinatorial structure will be considered as well as the simplifying submodular property of the objective function.
%Finally the \textit{robustness}  is a key issue in security games. We propose to insure robustness in  three different ways. First, one can be  very conservative and assume that an adversary actually chooses (in the most adversarial way) the data that we do not know. Second, we introduce offline learning  where the test and the learning happen  before the strategies is actually use in the real world. Third, we might be required to learn defence strategies that do not possess large variances in their performance.

This project will therefore deploy approaches of {\em statistics} and {\em machine learning} within the framework of security games which are played repeatedly between a defender and attackers. Repeated security games allow for the continuous collection of data, which can in turn be used to estimate the parameters of the game and influence future behavour. %Another possibility is in fact that the defender can in turn test the game and collect (in the most efficient and less costly manner) more information about the game. 
Our key objective is to design efficient and theoretically sound, data-driven methods that can actively interact with the environment to {\em learn} and {\em act} in security games of realistic scales, which must therefore be \textit{practical}, \textit{scalable} and \textit{robust}. 

 

 

\subsubsection*{State-of-the-art}
\paragraph{Security meets Game Theory }
%\TODO{better distinction between minimax nash stacjkelberg}
From a game-theoretic perspective, a security problem is viewed as a two-player game that captures the interaction between a defender (e.g., border patrols, metro inspectors, network administrators) and attackers (e.g., terrorists/smugglers, illegal metro users, malicious cyber attackers). The action of the defender (attacker) is defined as selecting a subset of targets to protect (attack). For each defender/attacker action pair, \textit{utilities} are defined as the players' gain or loss, and the players' objectives are to maximise their corresponding pay-offs. From the defender's perspective, this corresponds to efficiently allocating a limited number of resources to secure some predefined targets. The expected utilities for both players can be stored in two  matrices,  $\boldsymbol A$ for the defender,  $\boldsymbol B$ for the attacker. Entries $\boldsymbol A_{i,j}$ and  $\boldsymbol  B_{i,j}$  are the expected utilities for the defender and attacker, respectively, when the defender plays strategy $i$ and the attacker responds with strategy $j$.
%The defender allocates a limited number of resources to secure some predefined target, which may under threat by the attacker. 
Solutions to such games rely on randomised (mixed) strategies, making each player's behaviour unpredictable to the other.  If a fully competitive setting, in which the attacker's gain is the defender's loss (i.e.\ the so-called zero-sum games) a fully robust strategy can be calculated for the defender, in that it provides guaranteed performance against {\em any} possible attacker, even if the defender's strategy is completely revealed to the attacker.  
A generalisation to more general (non zero-sum) games, which is particularly relevant to security games, is the Stackelberg equilibrium\cite{korzhyk2011stackelberg} in which the defender's mixed strategy is first publicised and the attacker plays a best response to this mixed strategy.
It is this Stackelberg security game framework that will be considered in the proposed project.
% Indeed, this is well-suited to the scenario where the defender's strategies may be at risk of revelation, leak or discovery through repetitive interactions. 
%Beyond these fundamental results, 
%some questions forming the primarouy focus of research in security games have been scalability, robustness with respect to uncertainty on the players utilities, or devising strategies that take advantage of the attacker's potentially limited rationality or bounded memory\cite{tambe2012game}. 

%\noindent \textbf{\textit{\\Tools from Sequential decision-making under uncertainty}}\\

\paragraph{Uncertainty in Security Games}
Most standard game-theoretical analyses assume that the payoff matrices are known in advance.  However uncertainty is endemic in most real-world applications. 
%Unlike standard game-theoretical approaches, in some scenarios, the players is uncertain about their utilities (the $A_{i,j}$'s), or those of the other players'.
For instance, in the context of security games, the random selection of passengers for security checks at an airport is a source of uncertainty in this game, where the outcome is random and the probability of successful security enforcement  is unknown. 
As also confirmed by several empirical studies in fraud and cybercrime detection, this phenomenon can significantly decrease the defender's performance\cite{granick2005faking, swire2009no}.
Extensive studies have been dedicated to the design of security games that are robust with respect to uncertainty about the environment\cite{aghassi2006robust,Nguyen14RO, Kiekintveld:2013}. 
However, an important observation is that much more can be done in the case of {\em repeated} security games.  
Indeed, this repetition allows the defender to further reduce her uncertainty about the model and intelligently {\em learn} how to improve her performance over time. Specifically, in this case, a security game solver can autonomously take intelligent decisions at repeated instances of the game, by carefully collecting, extracting and acting upon information from historical data. As discussed further in below, this is precisely the area where mathematical machine learning has the strongest results.

%\noindent \textbf{\textit{\\iii. Indispensable Tools from Machine Learning}}\\
%Machine learning is a field of computer science where the goal is to design software able to extract information from data so that the machine itself can make use of this information to take  autonomous decisions.
%The problem of sequential decision-making arises in our everyday lives, when we seek answers to such simple questions as how to navigate to work, retrieve our desired information from the Internet, or even
%play and win at backgammon, poker or tetris (the latter was used as a testbed in the author's PhD thesis). Naturally, these situations incur uncertainty since not only are the dynamics of the environment stochastic but also because the underlying stochastic mechanism that generates these dynamics are unknown.   
%
%In the context of security games, the random selection of passengers for security checks at an airport is an example source of uncertainty in this game where the outcome is random and the probability of successful security enforcement  is unknown. 
%In these applications, a security game solver is required to autonomously take intelligent decisions at repeated instances of the game, and must thus carefully gather, extract and act upon information from historical data. 
%
\paragraph{Bandit problems}
Mathematical machine learning is a modern amalgamation of statistics (to deal with uncertain data) and optimisation (to efficiently select appropriate actions).   	
%It has proven to make a difference in practice and has nice mathematical background. We want to focus here on two particular methods that are interesting in machine learning and where Victor and David are experts.
%First, many interesting sequential decision-making tasks can be formulated as reinforcement learning (RL) problems. In RL, an agent interacts with a dynamic, stochastic, and incompletely known environment with the goal of learning a strategy or \textit{policy} to optimize some measure of its long-term performance (e.g.,~to remove as many lines as possible in Tetris).
%talk very quickly about MDP POMDP tree search?
One of the fundamental problems in machine learning, relevant to our research objectives in this proposal, is the \textit{multi-armed bandit problem}. It corresponds to a scenario where the learner is required to actively collect data from an environment in order to solve a given task. Solutions built for this problem have found many practical applications from adaptive routing in a network to medical trials of new medicines\cite{bubeck2012regret}.
A multi-armed bandit is precisely a game as described above, but where the attacker has only one action; the bandit problem results when the game is repeated, and on iteration $t$ the learner selects action $i(t)$ and received a reward $l_t$ which is a random variable with expectation equal to $\boldsymbol A_{i_t,1}$.
%In its simplest form there are $K$ actions (arms). At each round $t$, the environment allocates rewards to each arm (described as a vector $l^t\in\R^k$) while the learner simultaneously chooses an arm $I(t)$ to pull in order to obtain a reward $l^t_{I(t)}$.
An important constraint in this setting is that the player is not allowed to observe the hypothetical reward that would have been collected had another arm been selected instead.

In the bandit problem two related tasks are commonly considered.  One is the {\em online decision-making} task, in which the reward for each decision must be taken into account; this task is relevant to the immediate deployment of the system to actually make decisions while it learns.  The standard performance metric here is the regret, defined to be the difference between the total reward that could have been achieved if full information were available in advance, and the actual reward that was achieved.  The other task is that of {\em pure exploration}, in which a learning phase is permitted during which received rewards do not matter; this corresponds to allowing a training phase for the system prior to deployment.  The performance metric of this problem is the quality of the arm selected immediately after the training phase.  Application of the pure-exploration problem to parallel action selection in robotic planning has been extensively studied by Dr Gabillon \cite{Gabillon11MB}. 

When we consider more general security games, in which the attacker has more than one available action, we can instead think of a non-strategic attacker who uses a fixed distribution over actions through time.  This also corresponds to bandit problem for the attacker.
Bandit strategies are therefore important for active learning in security games.  However it is as yet an important open question how to take into account strategic or adaptive behaviour of the attacker.  It is this aspect of learning in security games which will be developed in the proposed project.
%This therefore corresponds to a simplified security games where the strategies of the other players are fixed and only an external environment allocates the rewards. One can think of a game where the $K$ arms/actions correspond to the $K$ possible security strategies whose values are determined according to some unknown underlying probability distribution. 
%The average per action reward can be estimated through sampling, i.e. via pulling the corresponding arm. 
%The problem has been studied under two main assumptions about the rewards. 
  
%all of the utilities are chosen by an  the defender takes actions against an attacker who adversarially designs the rewards of the game.
%For this proposal we focus on two different ways to measure the performance of the methods that corresponds to two different security requirements. % The first formulation corresponds to the classical cumulative regret setting where the forecaster tries to constantly choose the security strategy with the highest value on average.  The second one is the ``pure exploration'' setting, where the forecaster can uses the exploration phase, a limited phase during which he can freely pull the arms he chooses to, in order to identify the best  security strategy among the $K$.

%Sequential decision-making tasks is often coupled with online learning in the sense such problem needs to learn online how to solve a task while solving it. These two features, sequential decision making and online learning while be most required for our new problems right! elaborate a bit


%The problems put forward a notion of complexity of the problem. That we try to characterise well. And that we will try to study in our new problems.

%While the multi-arm bandit games are extremely indispensable to active learning, an important question is how they can be used in general security games where one has to also take into account the actions of the other players.


%\noindent \textbf{\textit{\\Frontiers of Security Games: From handling uncertainty towards self-learning algorithms}}\\
\paragraph{Existing results}
%An extensively litterature has tried to  devise security games methods that are robust with respect to uncertainty about the environment\cite{aghassi2006robust,Nguyen14RO, Kiekintveld:2013}.
%Granick, for example,
%argues that weaknesses in our understanding of the measurability of losses serve as
%an impediment in sentencing cybercrime offenders\cite{granick2005faking}. Swire adds that deterring
%fraudsters and criminals online is hampered if we cannot correctly aggregate their
%offences across different jurisdictions \cite{swire2009no}. These  interest in dealing with uncertainty in the data shows how important and crucial and costly it could be and that it is a real issue in real problems.
Recent advances have been made in this direction, which mostly focus on the case where the attacker's preferences are not fully known and must be learned through repeated plays of the game. One approach is to analyse the number of required queries to learn the optimal defender's strategy \cite{blum2014learning, letchford2009learning}. 
Alternatively a Bayesian approach can be taken, with techniques based on Partially Observable Markov Decision Processes (POMDPs) used to update a posterior over the adversary's preferences\cite{Marecki12PR, qian2014online}. 
%The main theoretical drawback of this planning method is in that the algorithm is based on Upper Confidence Trees (UCT), which, are provably sub-optimal\cite{munos2014bandits}. 
Recently, a more relevant analysis has been given\cite{Balcan15CR} for the case of multiple attackers, where at each round of the game, a single attacker is chosen adversarially from a fixed, finite, set of known attackers. This corresponds to a case where the utility matrix $\boldsymbol B$ is chosen adversarially from a set of $k$ known matrices, and shows strong connections with adversarial bandit theory.  However all of these security games results rely restrictive assumptions about prior knowledge and observability, and on the number of available actions to each player being reasonably small.  In this proposal we consider combinatorial actions (selecting $k$ resources to protect out of $n$ that may be attacked) and so the total number of actions available is enormous.

%\textbf{Motivation.} %%%V what can I bring

\paragraph{Vision}
%\noindent \textit{\textbf{\\Main Goal}}\\
%The main issues that have been considered so far in the literature are scalability, the ability for the designed methods to handle problems with very large number of actions for the players, or devising strategies that take advantage of the attacker's potentially limited rationality or bounded memory\cite{tambe2012game}.
The purpose of this project is to create \textit{practical}, \textit{scalable} and \textit{robust} methods for security games. First, we target \textit{practicality}  in the sense that our methods will be autonomous in handling uncertainty in the model and will actively reduce this uncertainty by interacting with the environment in which the game takes place during repeated plays of the game. We will do so by combining existing security game research with the extremely active field of multi-armed bandit research.
%Specifically, we aim to broaden the scope of repeated security game problems where the initial uncertainty about the players utilities can be overcome through using learning techniques  in conjunction with  repetitive plays of the game. 
%These techniques are from the extremely active field of machine learning research.
Second, we target \textit{scalability} so that the methods will apply with an extremely large number of possible actions. This can be achieved by making simplifying structure assumptions such as combinatorial structure, in which an action consists of selecting $k$ objects from $n$. % will be considered as well as the simplifying submodular property of the objective function.
Finally \textit{robustness}  is a key issue in security games in several senses.  Methods should not break down in the face of adversarial play by the attacker both during learning (so that the defender receives the least helpful information from which to learn).  Furthermore methods should not perform well only in average --- worst case performance is extremely important.  We will therefore devise a theoretically sound approach in which efficient algorithms are developed and finite time performance guarantees are provided.
%We propose to ensure robustness in  three different ways. First, one can be  very conservative and assume that an adversary actually chooses (in the most adversarial way) the data that we do not know. Second, we introduce offline learning  where the test and the learning happen  before the strategies is actually use in the real world. Third, we might be required to learn defence strategies that do not possess large variances in their performance.

%\textit{Efficient learning algorithms:} 
%Our goal is to have a theoretically sound approach by designing efficient algorithms for which we can provide finite sample analysis.%First we will look at new instances of these problem that are related to our area of bandit expertise and that serve real purpose.
%To solve real world problems our \textit{long-term} goals are:

%\begin{itemize}
%\item \textit{Explore stochastic and adversarial assumptions:} In a first setting, the environment chooses the rewards stochastically, based on some predetermined (however unknown) distributions. In security games this stochastic assumption helps model the unknown dynamics of the problem which both the defender and attacker can learn through interaction with the environment. In the second setting, no stochastic assumptions are made. Instead, an adversary is assumed to have arbitrarily chosen and designed the rewards
%\cite{Auer03NS}. This can be used to model a more pessimistic security game where the utilities are chosen adversarially by the attacker.
%Aside from contributing to adversarial setting, one of our interest  is to make Stochastic assumptions when dealing with noise in the model and adversarial assumption when dealing with the adversary to make our approach both realistic and robust.
%%%A depending on the considered scenraio, it may make sense to consider the problem as adversarial or stochastic. The distinction will be made clear in the ... 
%So far stochastic noise in the model  in conjunction with learning has been untouched while it can happen when phenomenon are not inherently adversarial (sensor that work stochastically action that have stochastic outcomes, checkpoint might not stop deterministically the attacks and the probability of success needs to be determined, here learned).
%Mention my previous work?




%The \textit{short-term} plan in achieving these goals is outlined in three main objectives below.




\textbf{Objective 1: Scalability in Pure exploration Bandits via Submodularity.}
When considering the combinatorial nature of many security games, it is necessary to use this structure intelligently.  In particular, naively enumerating all possible actions as in the standard formulation of security games makes the computations rapidly intractable.  
%As discussed previously, bandit problems extend naturally to repeated games under uncertainty. 
We will address this problem by first studying combinatorial bandit techniques, which will subsequently be extended to use in security games.  Combinatorial bandits form a central part of Dr Gabillon's area of expertise.   A key observation is that in many cases the players' performance utility function is submodular. One example is in the context of maximal coverage problem for sensor (or checkpoint) placement\cite{krause2011randomized}. 
 This submodularity property can in turn be used to provide tractable and almost optimal algorithms in the online decision-making settings of bandits\cite{gabillon2013adaptive}.  However pure exploration has not yet been solved in this area; Objective 1 is therefore to complete the analysis of combinatorial bandits by addressing the pure exploration problem under submodularity assumptions.
% 
% As discussed further in the objective 2, we are also interested the bandit formulation called pure exploration that will provide a safer scenario to tests the security strategies. As this bandit setting is part of the fellow expertise a very natural first step will be to consider combinatorial pure exploration under submodularity.
 
%  A numerous literature have been written on extending bandit approach to linear, combinatorial or continuous spaces. In this proposal we wil be more intereted in combinatorial assumption that aris ein security games  (games in networks, combinatorial actions). 
%HoweverRecently the fellow has proposed
 
 
\textbf{Objective 2: Pure Exploration in Security Games.}
%Contrary to the cumulative regret setting, the rewards collected before the end of the game are not taken into account. 
As discussed previously, a major barrier to applying security games to real-world scenarios is that the players' utility matrices $\boldsymbol A$ and $\boldsymbol B$ are unknown and must be learned. In the first of two objectives on standard security games we consider the case where the defender is in fact able to safely examine her defensive strategies before applying them online, corresponding to the pure exploration bandit framework. A familiar real-world example is the security system at an airport, which can be tested many times before being deployed as the principal defence scheme.  
%This corresponds to best-arm identification in the bandit problem.
%
%We observe that in this formulation of the problem, we can capture the learning module of the security game by a recent bandit setting called \textit{pure exploration}, where the learner is only evaluated at the end of an exploration phase comprised of a limited number of interactions with the environment. Pure exploration has generally proved useful in many practical scenarios. Specifically, its application to parallel action selection in robotic planning has been extensively studied by the fellow \cite{Gabillon11MB}. 
%However, applying this approach to security games is an open research path with a strong potential to bring new and interesting angles to the domain. 
In this formulation, we assume both utility matrices $\boldsymbol A$ and $\boldsymbol B$ unknown. During the test phase, the defender is able to probe an entry of its utility matrix at every mock repetition of the game. The value obtained is {\em a noisy version} of the true entry $\boldsymbol A_{i,j}$. 
%What makes this setting particularly well-suited to real-world security applications is in that, 
%1. unlike existing work, no assumption is required to be made about the defender's knowledge of its utility matrix $\boldsymbol A$. Instead, the utilities can be learned offline through probing the individual entries of $\boldsymbol A$, and 
%2. the stochastic framework gives a natural model for the players' uncertainty about the environment.
%
 
% \textbf{Complexity.}
 We will design a strategy for the defender to either minimise the number of tests needed to identify an excellent strategy with a given level of confidence\cite{Maron93HR,Even-Dar06AE} or to maximise her probability of identifying the best strategy given a fixed number of tests\cite{Bubeck09PE,Audibert10BA}. 
 %In the classical multi-armed bandit problem, these optimisation objectives respectively correspond to a \textit{fixed confidence} setting and a \textit{fixed budget} constraint .
 In order to extend these  classical results to the setting proposed above, we will first carefully characterise the data-dependent hardness of the problem, extending recent relevant results for combinatorial bandits\cite{chen2014combinatorial} and similar results currently in preparation by Dr Gabillon.  Of course, in games these complexity results are more challenging than in the bandit problem since the complexities depend also on the actions available to the attacker.
%  and then move on to designing algorithms to best capture the obtained hardness. In standard bandit, the complexity of an arm is inversely proportional to the gap $\Delta_i= \mu^*- \mu_i$ between the value of the best option $\mu^*$ and that of option $i$. Extensions of this notion have been designed for combinatorial bandits \cite{chen2014combinatorial}. The fellow is currently working on a improved version of the state-of-the-art result. The complexity of an arm defined in these combinatorial games is  more complex than in the simple multi-armed bandit problem as it involves combinatorial quantities. Similarly in security games, we expect the complexities of each entry of the matrix for the defender to depend also on the actions available to the attacker.
  Therefore we will study this problem by gradually increasing its difficulty with different partial feedback structures. First we note that a recent work\cite{goldberg2014query} on query complexity, corresponds to the simpler deterministic  version of this problem where it is assumed that the probing outcome corresponds to the {\em true value} of $\boldsymbol A_{i,j}$. %Thus they do not analyse the number of queries required to correctly estimate each entry of the utility matrix, which in turn depends on the performance of the strategies that would use this entry. 
Therefore our first approach will be to combine ideas from pure exploration and the deterministic query complexity setting in a context where the defender can individually sample from any entry of the matrix. A second more challenging setting will be to consider the more adversarial learning problem where the defender chooses a strategy and only observes the value of the game corresponding to an action selected by the attacker; the attacker might be either oblivious to the defender, or playing a Stackelberg best-response to the defender strategy.
 %Note that the hardness of the best arm identification problem in the stochastic setting can be interpreted as the total number of pulls required to discriminate the best arm(s) from the others. In a simple multi-arm bandit setting, this is defined as the sum of the complexity of each suboptimal option, where the complexity of a suboptimal option $i$ is inversely proportional to the gap $\Delta_i= \mu^*- \mu_i$ between the value of the best option $\mu^*$ and that of option $i$.
%More precisely the complexity $H$ is  defined as $H = \sum_{k} \frac{1}{\Delta_k^2}.$ 


%\textit{Different feedback structures:} The complexity of learning highly depends on the quality of the feedback that the learned can collect. From the most informative full information feedback to the many variations of partial feedback setting it is of importance, as discussed in Objective 1 below, to quantify how the nature of the feedback  affects the performance and to focus on scenario that correspond to real world examples.

 %
 %submodularity property of the problem that often arises in the context of sensors (or checkpoints) placement can provide tractable and almost optimal algorithms. Therefore submodular combinatorial pure exploration is a first step.



%One first step is to relax the problem as shown in Krause et al finding the best response to a given adversary. This is known to be is NP hard problem  but can be solve almost optimally be a greedy algorithm thank to a sub modularity property of the problem. This gives rise to a first objective which would be to learning optimise stochastic submodular function under a pure exploration setting.
%Note that I worked on similar subject with learning in submodular functions.



%The idea would be to  extend the work of Balcan using more complex bandit algorithms. They use a version with k known attackers. We can assume that k is extremely large but there is some  structure that permits us to use for instance combinatorial bandits.

%The security issue naturally has application in graph problem that model the network of roads/ connection between computers that agents might need to secure. Therefore there has been study that apply game theory to this problems. For instance it has been used to monitor road barrage in mumbai (connection) The goal is there to put some check point on a road to stop some terrorist. Its a one shot game where you try to minimise the probability of the player to pass.  Utilities are not really defined and complex here You just want to maximise the probability of catching the attacker. We are interested in a version of this game that is repeated . Everyday the same problem arises. We would minimise the cumulative regret. Therefore the defender can be adaptive and if the attacker is not smart and repeat always the same plan we will catch him often (not totally a worse case scenario). This can be seen actually has a specific problem of adversarial combinatorial bandits where the  attacker is limited to a very specific structure of losses which are path in a graph. We can expect to use the specificity of the graph by using some result from spectral graph theory. Maybe also we can use this theory to solve some issue with the scalability of the algorithm.

\textbf{Objective 3: Regret Analysis of Repeated Security Games.}
In some applications a newly created security system is not provided with any historical data and cannot be tested before being used in production. Here, the learning of the utilities must be performed online while actually playing the security game. In this online decision-making context it is of high importance for the agent to learn the utilities as fast as possible. This means that only providing an analysis to demonstrate asymptotic convergence\cite{LeslieCollins06,ChapmanEtAl2013} is inadequate. To address repetitive learning in security games we will here assume that the utility matrix $\boldsymbol  A$ is unknown to the defender and that, at each repetition of the game the attacker will best respond to the defender's current strategy (if $\pi$ is a mixed strategy of the defender, then denote $b(\pi)$ the best response of the attacker). Therefore we are considering a setting that is related to the analysis of Stackelberg equilibrium. A natural quantity of interest is the \textit{cumulative regret}, defined as
\begin{equation*}
R(n)=n\max_{\pi} \left[\pi A b(\pi)\right]-\sum_{t=1}^n \pi_t A b(\pi_t).
\end{equation*}
This compares the actual reward received (assuming the attacker always performs as well as they can) to the reward achieved at Stackelberg equilibrium (when the defender chooses the best possible mixed strategy under the knowledge that the attacker will best respond to it).  The objective is for the defender to build a series of defence strategies $\pi_t$ for $t=1,\ldots, n$ to minimize the expected cumulative regret $R(n)$.
% 
%
%where the v(G) is the value of the game when the two players play according to the Stackelberg equilibrium of the game. The cumulative regret would be therefore defined the difference between the sum of rewards collected by always using the \textit{best security strategy} in hindsight and the sum of rewards actually collected by the forecaster.
% This is an online game where, at each time set $t$ of the game, a fundamental trade-off arises for the forecaster between the simultaneous need to select the security strategy solution he currently thinks is the best in order to maximise the immediate security given his current knowledge  (exploitation)  while also wanting to test possible other strategies that might or might not be better (exploration).
 
 This game-theoretic scenario, is actually strongly related to the bandit problem, in that the best-responding assumption on the attacker leads to a situation where the reward to the defender depends only on the selected (mixed) strategy.  A solution can thus be obtained by considering it as a bandit problem with continuous action space consisting of the set of all probability distributions on the original discrete action space.  However a more efficient solution is likely to be obtained by explicitly considering the game-theoretical nature.
%
%Our goal here will be to provide a regret analysis for the online stochastic repeated security games. The first step will be to study  how to design a new algorithms borrowing ideas from the UCRL algorithm \cite{auer2009near} that was extending the bandits ideas of the classical UCB analysis to general reinforcement learning problems. 
In particular, most current approaches for online decision-making in bandits, such as upper confidence bound methods\cite{Auer02FA}, implement a strategy that is optimistic in face of uncertainty, playing any strategy that could be the best given the level of uncertainty. It will be extremely interesting to discover if this optimism principle still holds in an adversarial game, or whether a more cautious approach is needed.
%especially in a case where the adversary knows the true utilities of the games and moreover knows your levels of uncertainty.
%For this project collaboration with  Bruno Scherrer, a fellow's co-author, working at  Inria Nancy, would prove fruitful as he has recently analysis how reinforcement learning asymptotic analysis could be applied to zero sum games\cite{scherrerapproximate}.
Approaches based on Thompson sampling\cite{russo2014information} will be also considered as they have proved very efficient in practice and correspond to an area of expertise of Prof. Leslie.
%talk about the UCT stuff?

Finally an interesting additional requirement is to learn security strategies that are not only of good quality in average but also whose performance is not subject to large variance when used on a daily basis. This {\em risk-averse} requirement has been well-studied in the statistical community, % has a \textit{risk-averse} requirement. Recently this requirement 
and has recently been considered in a multi-armed bandit framework\cite{NIPS2012_4753}. An implementation in the security games specific context is a very natural extension to the main body of work in this objective.




\textbf{Objective 4:  Learning in combinatorial games.}
Objective 4 will be devoted to solving security games with more complex action structures. 
Real-world security problems often involve large, complex networks. This includes, for instance, complex routes, or computer/communication networks. The size of the action spaces for both attacker and defender is often combinatorially large.  Standard results with convergence times that increase with the size of the action spaces become extremely weak in such settings.
We will therefore take advantage of the inherent combinatorial structure of the problem to create efficient and computationally tractable algorithms in these large games.   In particular, we will develop the approaches of both Balcan\cite{Balcan15CR} in simple Stackelberg games and  in adversarial combinatorial bandits\cite{cesa2012combinatorial}.
The issue of scalability will be addressed in light of the results found in Objective 1.

\textbf{Objective 5:  Repeated Network-Security Games.}
As a more concrete application of Objective 4, this objective will focus on the particular combinatorial structure that is a graph as this stucture is present in  numerous real-word applications.
In light of the ever-growing, modern, social and communication networks, a canonical example is that of smuggler arrest in a network\cite{jain2011double}. This has received significant attention in the community, especially in  response  to  the  Mumbai  attacks  of  2008, after which  Mumbai  Police
started to schedule a limited number of inspection checkpoints
on the road throughout the city.
This problem has not yet been studied in its repeated form, where a pursuit-evasion game is played multiple times against a population of smugglers. Therefore, the currently deployed strategy of the defender is \textit{not adaptive} to observations collected about the attackers' historical strategy and is therefore sub-optimal.  We will therefore develop adaptive strategies, using the approaches developed in Objectives 1--4.  However, the graph structure provides additional constraints on the action spaces, and provides additional information, when compared with the general combinatorial problem.  In particular, the set of actions available to the attacker is restricted to a set of paths through the network, and the the graph structure provides strong information about sensible choices of checkpoints (for example, aligning them all along one route through the network is a particularly unfortunate choice, but is not ruled out by a generic combinatorial structure).  Furthermore, absence or presence of a smuggler on one link of the graph will likely provide information about which other other links were utilised on that iteration. Therefore the objective  is to design specific algorithms in in situations where it is possible to take advantage of specific graphical structure of the problem. We will start by defining a notion that captures the hardness of the task depending on characteristics of the graph that we would discover. Note that, although the goal is to generate algorithms for security games on graphs, the results to be obtained will be expected to lay grounds for research in a more general setting of active learning with graph structure. Dr Gabillon has held initial discussions on this topic with Dr Michal Valko, a world-famous expert in active learning on graphs and part of INRIA Lille in France. This project will allow the formation of a productive and lasting collaboration with Dr Valko, which will be greatly beneficial not only in achieving the this objective, but also to strengthen international links between Lancaster University in the UK and INRIA in France.  
 
 
 %A popular efficient  algorithm for it is Thompson Sampling as it has both proved very efficient\cite{Chapelle11EE}, handy and as recently started to be theoretically good~\mbox{\cite{Kaufmann12TS}}. 

%Note that a very numerous of variation of the initial games ranging from considering  infinite number of arms to continuous actions\cite{Wang08AI,Abbasi-Yadkori11IA,Dani07TP} permits these methods to adapt to a very large mount of challenges

%\noindent \textbf{\textit{\\Originality and innovative aspects of the research programme:}}\\
%This grant proposal is original has it proposes to develop a bridge between two areas of research (Security games and Machine learning) that have both proved their practical capacities but have only rarely been used jointly. By addressing some of the fundamental challenges of this connection we expect to bring more attention to the potential of this connection and bring the two communities to collaborate more in order to design software that solve important security problems.
%Our approach is to design algorithms are both theoretically grounded and of practical use in real world problems.
%Moreover by collaborating with with the Security Lancaster Departement in the University of Lancaster, we hope to apply and test our methods to real world problems.
% 
%\noindent \textbf{\textit{\\Timeliness and relevance:}}\\
%Security games has found numerous applications in the recent 10 years and has proved that it could bring a significant improvement over  systems designed by humans. Making this systems even more autonomous so that they can autonomously improve by constantly learning is a key issue to bring low maintenance security systems. We believe machine learning is providing most of the necessary tool to reach this goal. Machine Learning is one of the most rapidly growing community in computer science research as it is currently making a difference in key industrial sectors already  building strong recommendation systems, and being used by the most important companies of our time to create self-driving cars or new pattern recognition algorithms.   The connection between security games and machine learning is evident, work that have connects those two worlds are still rare and it will be the source of many new research challenges. 
%Finally bringing more robust security systems is of high importance for Europe has recent event have shown how ensuring the security of network communications from spy or securing public transportation is crucial.
% 